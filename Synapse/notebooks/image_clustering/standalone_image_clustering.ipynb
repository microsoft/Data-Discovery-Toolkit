{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License.\n",
        "\n",
        "# **This cell configures the spark session - Do not change**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-21T12:52:34.8346012Z",
              "execution_start_time": "2022-06-21T12:52:34.83431Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-21T12:51:54.6502908Z",
              "session_id": 27,
              "session_start_time": "2022-06-21T12:51:59.9090206Z",
              "spark_pool": null,
              "state": "finished",
              "statement_id": -1
            },
            "text/plain": [
              "StatementMeta(, 27, -1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.rpc.message.maxSize\": 1024,\n",
        "     \"spark.kryoserializer.buffer.max\": \"256m\"\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **These are the parameters that need to be changed to your values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-21T12:52:38.8747039Z",
              "execution_start_time": "2022-06-21T12:52:38.7312864Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-21T12:51:54.7085336Z",
              "session_id": 27,
              "session_start_time": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 1
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 27, 1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# The input directory where the images reside, can be nested\n",
        "input_directory = 'abfss://[].dfs.core.windows.net/animal_faces/walkthrough/'\n",
        "# The number of clusters - this can be automated or start with a guesstimate\n",
        "number_of_clusters = 3\n",
        "# The output directory where the output file will be written to\n",
        "output_directory = 'abfss://[].dfs.core.windows.net/animal_faces/output/'\n",
        "# The name of the output file\n",
        "output_filename = 'animal_faces_clustered_walkthrough_pca2.csv'\n",
        "\n",
        "# The blob account url - https://[accountname].blob.core.windows.net\n",
        "account_url = \"https://[].blob.core.windows.net\"\n",
        "# The blob account name = [accountname]\n",
        "account_name = ''\n",
        "# The blob account key [iufquq34r423r2==] - used to generate a SAS key\n",
        "account_key = ''\n",
        "\n",
        "# The name of the primary ADLS share\n",
        "file_system_name=\"share\"\n",
        "# The directory folders where your files reside  \n",
        "directory_name='animal_faces/walkthrough'  # bbc/videos/\n",
        "\n",
        "# If set to true generate a 3D scatterplot otherwise 2D\n",
        "SCATTER_PLOT_3D = False\n",
        "# If this is set to True then the Coalesce notebook will need to be run to merge the partition files into a single file\n",
        "LOW_MEMORY_MODE = True\n",
        "\n",
        "# AML Experiment tracking\n",
        "\n",
        "# Azure SubscriptionId\n",
        "subscription_id=\"\"\n",
        "# AzureML Workspace Resource Group\n",
        "resource_group=\"\"\n",
        "# AzureML Workspace Name\n",
        "workspace_name=\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Track the Experiment in Azure ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Workspace, Experiment, Run\n",
        "import mlflow\n",
        "\n",
        "ws = Workspace(subscription_id=subscription_id, resource_group=resource_group, workspace_name=workspace_name)    \n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "experiment_name = f\"({mssparkutils.runtime.context['notebookname']}_{str(mssparkutils.env.getJobId())})\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "mlflow.log_param(\"input_filename\", input_filename)\n",
        "mlflow.log_param(\"number_of_clusters\", number_of_clusters)\n",
        "mlflow.log_param(\"output_directory\", output_directory)\n",
        "mlflow.log_param(\"output_filename\", output_filename)\n",
        "mlflow.log_param(\"account_url\", account_url)\n",
        "mlflow.log_param(\"account_name\", account_name)\n",
        "mlflow.log_param(\"file_system_name\", file_system_name)\n",
        "mlflow.log_param(\"directory_name\", directory_name)\n",
        "mlflow.log_param(\"SCATTER_PLOT_3D\", SCATTER_PLOT_3D)\n",
        "mlflow.log_param(\"LOW_MEMORY_MODE\", LOW_MEMORY_MODE)\n",
        "params = {\n",
        "    \"sparkpool\": mssparkutils.runtime.context['sparkpool'],\n",
        "    \"workspace\": mssparkutils.runtime.context['workspace'],\n",
        "    \"notebookname\": mssparkutils.runtime.context['notebookname'],\n",
        "    \"isForPipeline\": mssparkutils.runtime.context['isForPipeline'],\n",
        "    \"pipelinejobid\": mssparkutils.runtime.context['pipelinejobid']\n",
        "}\n",
        "\n",
        "mlflow.log_params(params)\n",
        "mlflow.pyspark.ml.autolog()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-21T12:54:55.6561547Z",
              "execution_start_time": "2022-06-21T12:52:38.9881284Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-21T12:51:54.7830913Z",
              "session_id": 27,
              "session_start_time": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 2
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 27, 2, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-06-21 12:52:44.552054: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/functions.py:389: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import io\n",
        "import pandas as pd\n",
        "import ntpath\n",
        "import os\n",
        "\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import col, pandas_udf, lit, struct, PandasUDFType, udf\n",
        "import pyspark.sql.types as Types\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "\n",
        "from PIL import Image\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
        "\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import col, pandas_udf, lit, struct, PandasUDFType, udf\n",
        "import pyspark.sql.types as Types\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
        "from pyspark.ml import Pipeline\n",
        "            \n",
        "\n",
        "# Load all images\n",
        "images = spark.read.format(\"binaryFile\") \\\n",
        "  .option(\"recursiveFileLookup\", \"true\") \\\n",
        "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
        "  .load(input_directory)\n",
        "\n",
        "# Convert to spark df\n",
        "images_df = images.select(\n",
        "  col(\"path\"),\n",
        "  col(\"content\"))\n",
        "\n",
        "\n",
        "def get_filename(row):\n",
        "    return ntpath.basename(row)\n",
        "\n",
        "def preprocess(img_data):\n",
        "  try:\n",
        "    img = Image.open(io.BytesIO(img_data)).convert('RGB')\n",
        "    img = img.resize([299, 299])\n",
        "    x = np.asarray(img, dtype=\"float32\")\n",
        "  except OSError:\n",
        "    x = np.zeros((299, 299, 3))\n",
        "  return preprocess_input(x)\n",
        "\n",
        "def keras_model_udf(model_fn):\n",
        "  def predict(image_batch_iter):\n",
        "    model = model_fn()\n",
        "    for img_series in image_batch_iter:\n",
        "      processed_images = np.array([preprocess(img) for img in img_series])\n",
        "      predictions = model.predict(processed_images, batch_size=64)\n",
        "      predicted_labels = [x[0] for x in decode_predictions(predictions, top=1)]\n",
        "      results = []\n",
        "      for i, tuples in enumerate(predicted_labels):\n",
        "        all_predictions = tuples + (predictions[i],)\n",
        "        results.append(all_predictions)\n",
        "\n",
        "      yield pd.DataFrame(results)\n",
        "\n",
        "  return_type = \"class: string, desc: string, score:float, inceptionv3: array<float>\"\n",
        "  return pandas_udf(return_type, PandasUDFType.SCALAR_ITER)(predict)  \n",
        "\n",
        "def inceptionv3_fn():\n",
        "    model = InceptionV3(weights='imagenet')\n",
        "    model.set_weights(bc_model_weights.value)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = InceptionV3()\n",
        "bc_model_weights = sc.broadcast(model.get_weights())\n",
        "inceptionv3_udf = keras_model_udf(inceptionv3_fn)\n",
        "predictions = images_df.withColumn(\"preds\", inceptionv3_udf(col(\"content\")))\n",
        "\n",
        "\n",
        "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
        "df_with_vectors = predictions.select(\n",
        "    predictions[\"path\"],\n",
        "    predictions[\"content\"],\n",
        "    predictions[\"preds.desc\"], \n",
        "    predictions[\"preds.inceptionv3\"], \n",
        "    list_to_vector_udf(predictions[\"preds.inceptionv3\"]).alias(\"features\")\n",
        ")\n",
        "\n",
        "k = number_of_clusters \n",
        "pca_1 = PCA(k=20, inputCol=\"features\")\n",
        "pca_1.setOutputCol(\"pca_features\")\n",
        "\n",
        "if SCATTER_PLOT_3D:\n",
        "  pca_2 = PCA(k=3, inputCol=\"pca_features\")\n",
        "else:  \n",
        "  pca_2 = PCA(k=2, inputCol=\"pca_features\")\n",
        "  \n",
        "pca_2.setOutputCol(\"pca_scatterplot_features\")\n",
        "\n",
        "kmeans = KMeans(k=k, seed=42, initMode=\"k-means||\", distanceMeasure=\"cosine\")\n",
        "pipeline = Pipeline(stages=[pca_1, kmeans, pca_2])\n",
        "\n",
        "model = pipeline.fit(df_with_vectors)\n",
        "df_coords = model.transform(df_with_vectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-21T12:54:56.3516295Z",
              "execution_start_time": "2022-06-21T12:54:55.7916176Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-21T12:51:55.461003Z",
              "session_id": 27,
              "session_start_time": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 3
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 27, 3, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from datetime import datetime, timedelta\n",
        "from azure.storage.blob import BlobServiceClient, generate_container_sas, BlobSasPermissions\n",
        "token_credential = DefaultAzureCredential()\n",
        "\n",
        "blob_service_client = BlobServiceClient(\n",
        "    account_url=account_url,\n",
        "    credential=token_credential\n",
        ")\n",
        "\n",
        "from azure.storage.filedatalake import DataLakeServiceClient, generate_directory_sas\n",
        "SAS = generate_directory_sas(\n",
        "        account_name=account_name,\n",
        "        file_system_name=file_system_name,\n",
        "        directory_name=directory_name,\n",
        "        credential=account_key,\n",
        "        permission=BlobSasPermissions(read=True),\n",
        "        expiry=datetime.utcnow() + timedelta(days=100))\n",
        "\n",
        "SAS_key  = \"?\" + SAS\n",
        "\n",
        "storage_path = os.path.join(account_url, file_system_name, directory_name)\n",
        "SAS_path = []\n",
        "\n",
        "def build_sas_path(row):\n",
        "    file_name = ntpath.basename(row)\n",
        "    return account_url + \"/\" + file_system_name + \"/\" + directory_name + \"/\" + file_name + SAS_key\n",
        "\n",
        "udf_build_sas_path = udf(build_sas_path, StringType())\n",
        "\n",
        "def get_X(row):\n",
        "    return str(row.values[0])\n",
        "\n",
        "def get_Y(row):\n",
        "    return str(row.values[1])\n",
        "\n",
        "def get_Z(row):\n",
        "    return str(row.values[2])\n",
        "\n",
        "udf_get_X = udf(get_X, StringType())\n",
        "udf_get_Y = udf(get_Y, StringType())\n",
        "udf_get_Z = udf(get_Z, StringType())\n",
        "\n",
        "if SCATTER_PLOT_3D:\n",
        "    df_coords = df_coords.withColumn(\"blob_path\", udf_build_sas_path(df_coords.path)).withColumn(\"X\", udf_get_X(df_coords.pca_scatterplot_features).cast('string')).withColumn(\"Y\", udf_get_Y(df_coords.pca_scatterplot_features).cast('string')).withColumn(\"Z\", udf_get_Z(df_coords.pca_scatterplot_features).cast('string'))\n",
        "else:\n",
        "    df_coords = df_coords.withColumn(\"blob_path\", udf_build_sas_path(df_coords.path)).withColumn(\"X\", udf_get_X(df_coords.pca_scatterplot_features).cast('string')).withColumn(\"Y\", udf_get_Y(df_coords.pca_scatterplot_features).cast('string'))\n",
        "\n",
        "df_coords = df_coords.drop('inceptionv3', 'features','pca_features', 'content', 'pca_scatterplot_features')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-21T12:55:21.9118549Z",
              "execution_start_time": "2022-06-21T12:55:11.3843976Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-21T12:51:55.4649871Z",
              "session_id": 27,
              "session_start_time": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 5
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 27, 5, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if LOW_MEMORY_MODE:\n",
        "    df_coords.write.mode('overwrite').options(header='true').csv(os.path.join(output_directory, output_filename))\n",
        "else:\n",
        "    df_coords.coalesce(1).write.mode('overwrite').options(header='true').csv(os.path.join(output_directory, output_filename))\n",
        "\n",
        "mlflow.pyspark.ml.mlflow.end_run()"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
