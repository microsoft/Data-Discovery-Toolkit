{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License.\n",
        "\n",
        "# Azure Cognitive Search and the OpenAI RAG pattern in Azure Synapse/ Microsoft Fabric using the BBC Sports Dataset\n",
        "\n",
        "# To run this notebook, ensure you have run the deployment script in the Data Discovery Toolkit - [here](https://github.com/microsoft/Data-Discovery-Toolkit#if-you-do-not-have-a-synapse-workspace)\n",
        "\n",
        "## This notebook will showcase the following:\n",
        "\n",
        "- For a given dataset generate a query per record using a HuggingFace Question generator\n",
        "- Run the queries using Standard Search\n",
        "- Run the queries using Semantic Search\n",
        "- Run the queries using Standard Search and the OpenAI RAG pattern\n",
        "- Run the queries using Semantic Search and the OpenAI RAG pattern\n",
        "- Run the queries using Standard Hybrid Vector Search\n",
        "- Run the queries using Semantic Hybrid Vector Search\n",
        "- Run the queries using Standard Hybrid Vector Search and the OpenAI RAG pattern\n",
        "- Run the queries using Semantic Hybrid Vector Search and the OpenAI RAG pattern\n",
        "- Evaluate the search results and use BART for OpenAI answer Entailment evaluation\n",
        "- Evaluate using a normalised score for both semantic and standard search\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\n",
        "## This cell configures the spark session - Do not change (not needed for Trident)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-29T08:31:19.5723891Z",
              "execution_start_time": "2023-06-29T08:31:19.5721177Z",
              "livy_statement_state": "available",
              "parent_msg_id": "c1eda425-7e0e-4ada-9e0c-ffb8fb545238",
              "queued_time": "2023-06-29T08:27:42.9844701Z",
              "session_id": "92",
              "session_start_time": "2023-06-29T08:27:43.056739Z",
              "spark_jobs": null,
              "spark_pool": null,
              "state": "finished",
              "statement_id": -1
            },
            "text/plain": [
              "StatementMeta(, 92, -1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "     \"spark.rpc.message.maxSize\": 1024,\n",
        "     \"spark.kryoserializer.buffer.max\": \"1024m\"\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-29T08:32:48.7106953Z",
              "execution_start_time": "2023-06-29T08:32:19.2363168Z",
              "livy_statement_state": "available",
              "parent_msg_id": "0bc2684c-d699-4979-a390-f540fe06e52b",
              "queued_time": "2023-06-29T08:31:55.7712184Z",
              "session_id": "92",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 2
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 92, 2, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-29 08:32:28.753724: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "import sys\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql import SparkSession\n",
        "import ntpath\n",
        "import os\n",
        "import numpy as np\n",
        "import openai\n",
        "from math import sqrt\n",
        "from scipy.stats import spearmanr\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from requests import post, put\n",
        "\n",
        "\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql.functions import udf,col\n",
        "\n",
        "from itertools import combinations\n",
        "from operator import itemgetter\n",
        "\n",
        "from graphframes import *\n",
        "from pyspark.sql.functions import monotonically_increasing_id, lit\n",
        "\n",
        "\n",
        "import os\n",
        "import openai\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.models import QueryType\n",
        "from azure.search.documents.indexes import SearchIndexClient  \n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "import spacy\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from fuzzywuzzy import fuzz\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, IntegerType, StructType\n",
        "from pyspark.sql.functions import udf,col\n",
        "from objdict import ObjDict\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        " \n",
        "import requests\n",
        "from pprint import pprint\n",
        "import json\n",
        "import ntpath\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Update all your parameters here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-29T08:48:07.8259458Z",
              "execution_start_time": "2023-06-29T08:48:07.666372Z",
              "livy_statement_state": "available",
              "parent_msg_id": "6f9de8c4-bc0e-44c9-9310-ccc116583522",
              "queued_time": "2023-06-29T08:48:07.5126374Z",
              "session_id": "92",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 19
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 92, 19, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# OpenAI deployment values\n",
        "global openai_chatgpt_deployment\n",
        "openai_chatgpt_deployment = \"chat\"\n",
        "\n",
        "global openai_gpt_deployment\n",
        "openai_gpt_deployment = \"davinci\"\n",
        "\n",
        "global openai_service\n",
        "openai_service = \"\"\n",
        "\n",
        "global openai_api_key\n",
        "openai_api_key = \"\"\n",
        "\n",
        "global openai_api_type\n",
        "openai_api_type = \"azure\"\n",
        "\n",
        "global openai_api_base\n",
        "openai_api_base = f\"https://{openai_service}.openai.azure.com\"\n",
        "\n",
        "global openai_api_version\n",
        "openai_api_version = \"2022-12-01\"\n",
        "\n",
        "global openai_embeddings_deployment_id\n",
        "openai_embeddings_deployment_id = \"te\" \n",
        "\n",
        "# If you have a RPM rate limit on your OpenAI account - sleep time in seconds\n",
        "openai_sleep_time = 120\n",
        "\n",
        "# The input file name that you want to query and egenerate questions from\n",
        "input_filename = 'abfss://share@datadiscoverypipeline2.dfs.core.windows.net/bbcsports/csv/sport_articles.csv'\n",
        "\n",
        "# The vector file - pre-extracted\n",
        "input_vector_file = 'abfss://share@datadiscoverypipeline2.dfs.core.windows.net/bbcsports/csv/bbcVectors.json'\n",
        "\n",
        "# How many records to query and evaluate\n",
        "global top\n",
        "top = 3\n",
        "\n",
        "# Azure Search Admin Key\n",
        "global search_admin_key\n",
        "search_admin_key = \"\" \n",
        "# The name of the search service\n",
        "global search_service_name\n",
        "search_service_name = \"dd28\" \n",
        "# The Azure Search Query Key\n",
        "global search_query_key\n",
        "search_query_key = \"\" \n",
        "\n",
        "global search_index\n",
        "search_index = \"bbcvector\"\n",
        "# This is the name of the semantic configuration on the search index\n",
        "global semantic_configuration_name\n",
        "semantic_configuration_name = \"bbc-semantic-config2\"\n",
        "global vector_search_api_version\n",
        "vector_search_api_version = \"2023-07-01-Preview\"\n",
        "\n",
        "\n",
        "search_client = SearchClient(\n",
        "    endpoint=f\"https://{search_service_name}.search.windows.net\",\n",
        "    index_name=search_index,\n",
        "    credential=AzureKeyCredential(search_admin_key))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Test OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-29T08:48:17.6841956Z",
              "execution_start_time": "2023-06-29T08:48:15.7998227Z",
              "livy_statement_state": "available",
              "parent_msg_id": "b0099f68-67c5-4adc-861e-50aeceb57230",
              "queued_time": "2023-06-29T08:48:15.661716Z",
              "session_id": "92",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 21
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 92, 21, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "I am a recent college graduate with a bachelor's degree in marketing. I am a hard-working, dedicated, and reliable individual who enjoys taking on new challenges and learning new skills. I am also an organized and detail-oriented person, with excellent communication and interpersonal skills, which I believe will be a great asset in any role. I am passionate about marketing and am eager to use my knowledge and skills to help businesses reach their goals.\n"
          ]
        }
      ],
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "def generate_embeddings(text):\n",
        "    response = openai.Embedding.create(\n",
        "        input=text, deployment_id=openai_embeddings_deployment_id)\n",
        "    embeddings = response['data'][0]['embedding']\n",
        "    return embeddings\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "def generate_completion(engine, prompt, temperature, max_tokens, stop):\n",
        "    completion = openai.Completion.create(\n",
        "                    engine=openai_gpt_deployment,\n",
        "                    prompt=prompt,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=max_tokens,\n",
        "                    stop=stop)\n",
        "    return completion.choices[0].text    \n",
        "\n",
        "openai_embeddings_deployment_id = openai_embeddings_deployment_id\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "openai.api_type = openai_api_type\n",
        "openai.api_base = f\"https://{openai_service}.openai.azure.com\"\n",
        "openai.api_version = openai_api_version\n",
        "\n",
        "search = generate_completion(engine=openai_chatgpt_deployment, prompt=\"Tell me about yourself\", temperature=0.7, max_tokens=1024, stop=[\"<|im_end|>\", \"<|im_start|>\"])\n",
        "assert len(search) != 0\n",
        "print(search)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\n",
        "# Let's automatically generate a query or each record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-29T08:48:44.9815352Z",
              "execution_start_time": "2023-06-29T08:48:25.2117688Z",
              "livy_statement_state": "available",
              "parent_msg_id": "883190b4-2bb5-4e3e-9c10-cf229fc1b881",
              "queued_time": "2023-06-29T08:48:25.0446984Z",
              "session_id": "92",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 22
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 92, 22, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d91fc70-283b-41a7-ba66-4500c360ee16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/331 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "160e7ee9-b09b-41ad-b33a-275959d6f454",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "309e4640-575f-4d41-913c-b16546679314",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b43412d-fca2-4ac0-886a-15bb8e275a17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5092715c-b50c-425d-b1f9-513a310ea136",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3deffb5e-b7f9-4148-8de8-3d559bb0e833",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24e78189-9234-469c-9783-948706cbc447",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/712M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-29T08:48:45.4528421Z",
              "execution_start_time": "2023-06-29T08:48:45.2216534Z",
              "livy_statement_state": "available",
              "parent_msg_id": "45e4227d-aa67-499a-bd6a-3ec3dd8d3482",
              "queued_time": "2023-06-29T08:48:27.1402302Z",
              "session_id": "92",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 23
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 92, 23, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "model_name = \"voidful/context-only-question-generator\" #\"allenai/t5-small-squad2-question-generation\"\n",
        "\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "qa_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "def generate_question(input_string):\n",
        "    input_ids = qa_tokenizer.encode(input_string, return_tensors=\"pt\", truncation=True)\n",
        "    res = qa_model.generate(input_ids)\n",
        "    output = qa_tokenizer.batch_decode(res, skip_special_tokens=True)\n",
        "    return str(output[0])\n",
        "\n",
        "udf_generate_question = udf(generate_question, StringType()) \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Test the question generation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-23T10:28:37.9672471Z",
              "execution_start_time": "2023-06-23T10:28:36.0697969Z",
              "livy_statement_state": "available",
              "parent_msg_id": "540ffe9b-5d83-49e3-b208-e6e4f0bcbff4",
              "queued_time": "2023-06-23T10:28:35.9164339Z",
              "session_id": "84",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 15
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 84, 15, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Who won the Rugby World Cup semi-final three years ago?\n"
          ]
        }
      ],
      "source": [
        "context = \"\"\"Eddie Jones says New Zealand are vulnerable as his England team prepares to face the three-time world champions for the first time since 2019.\n",
        "\n",
        "England's victory in the Rugby World Cup semi-final three years ago is a highlight of Jones' seven-year reign.\n",
        "\n",
        "\"We showed that if you've got the right attitude and the right game plan then history can be broken,\" Jones said.\n",
        "\n",
        "\"If we go after them then they're there for the taking - and we're going to go after them.\"\n",
        "\n",
        "New Zealand, who play Scotland on Sunday, visit Twickenham next Saturday, with England coming into the game on the back of a 52-13 win over Japan.\n",
        "\n",
        "New Zealand have won their last five matches, but lost six of the previous eight. That run came close to costing Jones\"\"\"\n",
        "question = generate_question(context)\n",
        "\n",
        "assert question == \"Who won the Rugby World Cup semi-final three years ago?\"\n",
        "print(question)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run the Question generation process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-11T09:13:43.017674Z",
              "execution_start_time": "2023-06-11T09:13:38.9528686Z",
              "livy_statement_state": "available",
              "parent_msg_id": "c4737f9b-afac-4b31-8405-a6ed34e59f66",
              "queued_time": "2023-06-11T09:13:38.8012282Z",
              "session_id": "40",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 29
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 40, 29, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_questions = df.withColumn(\"question\", udf_generate_question(col(\"text\")))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Test the Search client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-27T12:40:46.1719154Z",
              "execution_start_time": "2023-06-27T12:40:45.0321223Z",
              "livy_statement_state": "available",
              "parent_msg_id": "983a0a3d-edac-4566-8dce-182e90c6c056",
              "queued_time": "2023-06-27T12:40:44.8502612Z",
              "session_id": "90",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 9
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 90, 9, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rugby008.txt\n"
          ]
        }
      ],
      "source": [
        "r = search_client.search(\"Whose thumb was fractured?\",\n",
        "                                 #filter=filter,\n",
        "                                 query_type=\"semantic\",\n",
        "                                 query_language=\"en-us\",\n",
        "                                 query_speller=\"lexicon\",\n",
        "                                 semantic_configuration_name=semantic_configuration_name,\n",
        "                                 top=top)\n",
        "\n",
        "\n",
        "\n",
        "results = [doc for doc in r]\n",
        "for i, doc in enumerate(results):\n",
        "\n",
        "    break\n",
        "\n",
        "assert doc['file'] == \"rugby008.txt\"\n",
        "print(doc['file'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load the vectors file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-29T08:32:52.9236244Z",
              "execution_start_time": "2023-06-29T08:32:48.8821888Z",
              "livy_statement_state": "available",
              "parent_msg_id": "0944bf0a-1532-48da-836a-d9a7164a4e5d",
              "queued_time": "2023-06-29T08:32:05.8456754Z",
              "session_id": "92",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 3
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 92, 3, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def read_batch_config(batch_root: str):\n",
        "    \"\"\"\n",
        "    We read the config file using the Java File System API as we do not need to let multiple nodes read individual lines and join it\n",
        "    all back together again\n",
        "    \"\"\"\n",
        "    # Change our file system from 'synapse' to 'input'\n",
        "    sc._jsc.hadoopConfiguration().set(\"fs.defaultFS\", ntpath.dirname(input_vector_file))\n",
        "\n",
        "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
        "    config_path = sc._jvm.org.apache.hadoop.fs.Path(f'{batch_root}')\n",
        "\n",
        "    # If we don't have a batch config, copy the global one.\n",
        "    if fs.exists(config_path) != True:\n",
        "        logger.error(f'{config_path} not found.')\n",
        "\n",
        "    # Open our file directly rather than through spark\n",
        "    input_stream = fs.open(config_path)  # FSDataInputStream\n",
        "\n",
        "    config_string = sc._jvm.java.io.BufferedReader(\n",
        "        sc._jvm.java.io.InputStreamReader(input_stream, sc._jvm.java.nio.charset.StandardCharsets.UTF_8)\n",
        "        ).lines().collect(sc._jvm.java.util.stream.Collectors.joining(\"\\n\"))\n",
        "\n",
        "    # Load it into json    \n",
        "    return json.loads(config_string)\n",
        "\n",
        "vectors = read_batch_config(input_vector_file)\n",
        "assert vectors is not None\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create the Vector Index - use REST to avoid SDK dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-29T08:44:51.1128988Z",
              "execution_start_time": "2023-06-29T08:44:43.9900092Z",
              "livy_statement_state": "available",
              "parent_msg_id": "20ce40cc-b4fb-4094-a43e-e7dd3cbe45ea",
              "queued_time": "2023-06-29T08:44:43.8410379Z",
              "session_id": "92",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 17
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 92, 17, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success creating index\n",
            "Success uploading vectors and data to index\n"
          ]
        }
      ],
      "source": [
        "headers = {\n",
        "    \"api-key\": search_admin_key,     \n",
        "    \"Content-Type\": \"application/json\",\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "body = {\n",
        "        \"name\": search_index,\n",
        "        \"fields\": [\n",
        "        {\n",
        "            \"name\": \"id\",\n",
        "            \"type\": \"Edm.String\",\n",
        "            \"key\": True,\n",
        "            \"filterable\": True\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"file\",\n",
        "            \"type\": \"Edm.String\",\n",
        "            \"searchable\": True,\n",
        "            \"retrievable\": True\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"content\",\n",
        "            \"type\": \"Edm.String\",\n",
        "            \"searchable\": True,\n",
        "            \"retrievable\": True\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"category\",\n",
        "            \"type\": \"Edm.String\",\n",
        "            \"filterable\": True,\n",
        "            \"searchable\": True,\n",
        "            \"retrievable\": True\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"contentVector\",\n",
        "            \"type\": \"Collection(Edm.Single)\",\n",
        "            \"searchable\": True,\n",
        "            \"retrievable\": True,\n",
        "            \"dimensions\": 1536,\n",
        "            \"vectorSearchConfiguration\": \"bbc-vector-config2\"\n",
        "        }\n",
        "    ],\n",
        "    \"corsOptions\": {\n",
        "        \"allowedOrigins\": [\n",
        "            \"*\"\n",
        "        ],\n",
        "        \"maxAgeInSeconds\": 60\n",
        "    },\n",
        "    \"vectorSearch\": {\n",
        "        \"algorithmConfigurations\": [\n",
        "            {\n",
        "                \"name\": \"bbc-vector-config2\",\n",
        "                \"kind\": \"hnsw\",\n",
        "                \"hnswParameters\": {\n",
        "                    \"m\": 4,\n",
        "                    \"efConstruction\": 400,\n",
        "                    \"efSearch\": 1000,\n",
        "                    \"metric\": \"cosine\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "    \"semantic\": {\n",
        "        \"configurations\": [\n",
        "            {\n",
        "                \"name\": \"bbc-semantic-config2\",\n",
        "                \"prioritizedFields\": {\n",
        "                    \"prioritizedContentFields\": [\n",
        "                        {\n",
        "                            \"fieldName\": \"content\"\n",
        "                        }\n",
        "                    ],\n",
        "                    \"prioritizedKeywordsFields\": [\n",
        "                        {\n",
        "                            \"fieldName\": \"category\"\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Create the index\n",
        "try:\n",
        "    url = f\"https://{search_service_name}.search.windows.net/indexes?api-version={vector_search_api_version}\"\n",
        "    resp = post(url=url, json=body, headers=headers)\n",
        "    \n",
        "    result_response = resp.json()\n",
        "    if resp.status_code == 403:\n",
        "        print(\"Authorisation Failed: Check that your API KEY value is correct\")\n",
        "        \n",
        "    if resp.status_code == 400:\n",
        "        print(f\"Error\", resp.text)    \n",
        "            \n",
        "    if resp.status_code == 201:\n",
        "        print(\"Success creating index\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print('Exception creating index', e)\n",
        "\n",
        "assert resp.status_code == 201\n",
        "\n",
        "# Upload the vectors\n",
        "\n",
        "vector_data = {\n",
        "                \"value\": vectors,\n",
        "                \"@search.action\": \"upload\"\n",
        "               }\n",
        "                \n",
        "\n",
        "\n",
        "try:\n",
        "    url = f\"https://{search_service_name}.search.windows.net/indexes/{search_index}/docs/index?api-version={vector_search_api_version}\"\n",
        "    resp = post(url=url, json=vector_data, headers=headers)\n",
        "    \n",
        "    result_response = resp.json()\n",
        "    if resp.status_code == 403:\n",
        "        print(\"Authorisation Failed: Check that your API KEY value is correct\")\n",
        "        \n",
        "    if resp.status_code == 400:\n",
        "        print(f\"Error\", resp.text)    \n",
        "            \n",
        "    if resp.status_code == 200:\n",
        "        print(\"Success uploading vectors and data to index\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print('Exception creating index', e)\n",
        "\n",
        "assert resp.status_code == 200"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step 1 Run Semantic search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-27T12:49:46.7419638Z",
              "execution_start_time": "2023-06-27T12:45:56.7339642Z",
              "livy_statement_state": "available",
              "parent_msg_id": "eb15e2ce-ed97-425f-bc0f-f6b4b3901d52",
              "queued_time": "2023-06-27T12:45:56.5304897Z",
              "session_id": "90",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 17
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 90, 17, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#1\n",
        "\n",
        "def evaluateSemantic(input_string):\n",
        "\n",
        "    r = search_client.search(input_string,\n",
        "                                    #filter=filter,\n",
        "                                    query_type=\"semantic\",\n",
        "                                    query_language=\"en-us\",\n",
        "                                    query_speller=\"lexicon\",\n",
        "                                    semantic_configuration_name=semantic_configuration_name,\n",
        "                                    top=top)\n",
        "    output = []\n",
        "    values = {}\n",
        "\n",
        "    for i, doc in enumerate(r):\n",
        "        values['\"'+doc['file']+'\"'] = ['\"'+doc['id']+'\"', '\"'+str(doc['@search.score'])+'\"', '\"'+str(doc['@search.reranker_score'])+'\"', '\"'+str(i)+'\"']\n",
        "    output.append(values)\n",
        "    return output \n",
        "\n",
        "udf_evaluateSemantic = udf(evaluateSemantic, ArrayType(StringType()))\n",
        "\n",
        "df_questions = df_questions.withColumn(\"SemanticSearch\", udf_evaluateSemantic(col(\"question\")))\n",
        "df_questions.write.saveAsTable(\"2eval_step1\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step 2 Run Standard Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-27T13:05:11.9403191Z",
              "execution_start_time": "2023-06-27T13:00:14.0841418Z",
              "livy_statement_state": "available",
              "parent_msg_id": "a758298f-3d2d-420c-8b19-25d7aef8f4a8",
              "queued_time": "2023-06-27T13:00:13.9425597Z",
              "session_id": "90",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 19
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 90, 19, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#2\n",
        "def evaluateSimpleSearch(input_string):\n",
        "\n",
        "    r = search_client.search(input_string, top=top)\n",
        "\n",
        "    output = []\n",
        "    values = {}\n",
        "    \n",
        "    for i, doc in enumerate(r):\n",
        "        values['\"'+doc['file']+'\"'] = ['\"'+doc['id']+'\"', '\"'+str(doc['@search.score'])+'\"', '\"'+str(doc['@search.reranker_score'])+'\"', '\"'+str(i)+'\"']\n",
        "    output.append(values)\n",
        "    return output \n",
        "\n",
        "\n",
        "\n",
        "udf_evaluateSimpleSearch = udf(evaluateSimpleSearch, ArrayType(StringType()))\n",
        "\n",
        "\n",
        "df_questions = df_questions.withColumn(\"SimpleSearch\", udf_evaluateSimpleSearch(col(\"question\")))\n",
        "df_questions.write.saveAsTable(\"2eval_step2\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step 3 Standard Search RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-27T13:37:42.5868309Z",
              "execution_start_time": "2023-06-27T13:15:56.131855Z",
              "livy_statement_state": "available",
              "parent_msg_id": "6fa19b1c-8187-43ee-aa48-f3eb3021718e",
              "queued_time": "2023-06-27T13:15:55.9246052Z",
              "session_id": "90",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 24
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 90, 24, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#3\n",
        "\n",
        "def evaluateSimpleSearchRAG(user_input):\n",
        "\n",
        "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "    def generate_completion(engine, prompt, temperature, max_tokens, stop):\n",
        "        completion = openai.Completion.create(\n",
        "                        engine=engine,\n",
        "                        prompt=prompt,\n",
        "                        temperature=temperature,\n",
        "                        max_tokens=max_tokens,\n",
        "                        stop=stop)\n",
        "        return completion.choices[0].text    \n",
        "\n",
        "\n",
        "    error_string = \"\"\n",
        "    has_error = False\n",
        "\n",
        "    openai.api_key = openai_api_key\n",
        "    openai.api_type = openai_api_type\n",
        "    openai.api_base = f\"https://{openai_service}.openai.azure.com\"\n",
        "    openai.api_version = openai_api_version\n",
        "\n",
        "    prompt_prefix = \"\"\"<|im_start|>system\n",
        "    Let's work this out it a step by step to be sure we have the right answer\n",
        "\n",
        "    Sources:\n",
        "    {sources}\n",
        "    \n",
        "\n",
        "    <|im_end|>\"\"\"\n",
        "\n",
        "    turn_prefix = \"\"\"\n",
        "    <|im_start|>user\n",
        "    \"\"\"\n",
        "\n",
        "    turn_suffix = \"\"\"\n",
        "    <|im_end|>\n",
        "    <|im_start|>assistant\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_history = turn_prefix\n",
        "\n",
        "    history = []\n",
        "\n",
        "    summary_prompt_template = \"\"\"Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base. Generate a search query based on the conversation and the new question. Source names are not good search terms to include in the search query.\n",
        "\n",
        "    Summary:\n",
        "    {summary}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Search query:\n",
        "    \"\"\"\n",
        "\n",
        "    content = \"\"\n",
        "\n",
        "    # Exclude category, to simulate scenarios where there's a set of docs you can't see\n",
        "    exclude_category = None\n",
        "\n",
        "    if len(history) > 0:\n",
        "\n",
        "        try:\n",
        "            search = generate_completion(engine=openai_gpt_deployment, prompt=summary_prompt_template.format(summary=\"\\n\".join(history), question=user_input), temperature=0.9, max_tokens=320, stop=[\"\\n\"])\n",
        "        except Exception as e:\n",
        "            has_error = True\n",
        "            error_string = e\n",
        "    else:\n",
        "        search = user_input\n",
        "\n",
        "        # Alternatively simply use search_client.search(q, top=3) if not using semantic search\n",
        "        filter = \"category ne '{}'\".format(exclude_category.replace(\"'\", \"''\")) if exclude_category else None\n",
        "        r = search_client.search(search,\n",
        "                                 filter=filter,\n",
        "                                 top=top)\n",
        "\n",
        "        search_results = [doc for doc in r]\n",
        "        results = [doc['content'][:2000].replace(\"\\n\", \"\").replace(\"\\r\", \"\") for doc in search_results]\n",
        "\n",
        "    prompt = prompt_prefix.format(sources=results) + prompt_history + user_input + turn_suffix\n",
        "\n",
        "    try:\n",
        "        completion = \"\"\n",
        "        completion = generate_completion(engine=openai_gpt_deployment, prompt=prompt, temperature=0.7, max_tokens=1024, stop=[\"<|im_end|>\", \"<|im_start|>\"])\n",
        "        completion = completion.replace('\"', \"\").replace(\",\", \"\").replace(\"'\", \"\")\n",
        "    except Exception as e:\n",
        "        has_error = True\n",
        "        error_string = e\n",
        "\n",
        "    # Build the search output\n",
        "    output = []\n",
        "    values = {}\n",
        "\n",
        "    for i, doc in enumerate(search_results):\n",
        "        values['\"'+doc['file']+'\"'] = ['\"'+doc['id']+'\"', '\"'+str(doc['@search.score'])+'\"', '\"'+str(doc['@search.reranker_score'])+'\"', '\"'+str(i)+'\"', '\"'+completion+'\"']\n",
        "    output.append(values)\n",
        "\n",
        "    if has_error == False:\n",
        "        prompt_history += user_input + turn_suffix + completion + \"\\n<|im_end|>\" + turn_prefix\n",
        "        history.append(\"user: \" + user_input)\n",
        "        history.append(\"assistant: \" + completion)\n",
        "\n",
        "    if has_error == True:\n",
        "        return str(error_string)\n",
        "    else:\n",
        "\n",
        "        return output\n",
        "\n",
        "udf_evaluateSimpleSearchRAG = udf(evaluateSimpleSearchRAG, ArrayType(StringType()))\n",
        "df_questions = df_questions.withColumn(\"SimpleSearchRAG\", udf_evaluateSimpleSearchRAG(col(\"question\")))\n",
        "df_questions.write.saveAsTable(\"2eval_step3\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step 4 Semantic Search RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-27T14:30:03.8236816Z",
              "execution_start_time": "2023-06-27T13:50:56.9278136Z",
              "livy_statement_state": "available",
              "parent_msg_id": "118a17c2-3f1e-47a8-a178-a17167f20364",
              "queued_time": "2023-06-27T13:50:56.7534174Z",
              "session_id": "90",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 26
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 90, 26, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#4\n",
        "\n",
        "def evaluateSemanticSearchRAG(user_input):\n",
        "\n",
        "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "    def generate_completion(engine, prompt, temperature, max_tokens, stop):\n",
        "        completion = openai.Completion.create(\n",
        "                        engine=engine,\n",
        "                        prompt=prompt,\n",
        "                        temperature=temperature,\n",
        "                        max_tokens=max_tokens,\n",
        "                        stop=stop)\n",
        "        return completion.choices[0].text    \n",
        "\n",
        "\n",
        "    error_string = \"\"\n",
        "    has_error = False\n",
        "\n",
        "    openai.api_key = openai_api_key\n",
        "    openai.api_type = openai_api_type\n",
        "    openai.api_base = f\"https://{openai_service}.openai.azure.com\"\n",
        "    openai.api_version = openai_api_version\n",
        "\n",
        "    prompt_prefix = \"\"\"<|im_start|>system\n",
        "    Let's work this out it a step by step to be sure we have the right answer\n",
        "\n",
        "    Sources:\n",
        "    {sources}\n",
        "    \n",
        "\n",
        "    <|im_end|>\"\"\"\n",
        "\n",
        "    turn_prefix = \"\"\"\n",
        "    <|im_start|>user\n",
        "    \"\"\"\n",
        "\n",
        "    turn_suffix = \"\"\"\n",
        "    <|im_end|>\n",
        "    <|im_start|>assistant\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_history = turn_prefix\n",
        "\n",
        "    history = []\n",
        "\n",
        "    summary_prompt_template = \"\"\"Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base. Generate a search query based on the conversation and the new question. Source names are not good search terms to include in the search query.\n",
        "\n",
        "    Summary:\n",
        "    {summary}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Search query:\n",
        "    \"\"\"\n",
        "\n",
        "    content = \"\"\n",
        "\n",
        "    # Exclude category, to simulate scenarios where there's a set of docs you can't see\n",
        "    exclude_category = None\n",
        "\n",
        "    if len(history) > 0:\n",
        "\n",
        "        try:\n",
        "            search = generate_completion(engine=openai_gpt_deployment, prompt=summary_prompt_template.format(summary=\"\\n\".join(history), question=user_input), temperature=0.9, max_tokens=320, stop=[\"\\n\"])\n",
        "        except Exception as e:\n",
        "            has_error = True\n",
        "            error_string = e\n",
        "\n",
        "        print(f\"search {search}\")\n",
        "    else:\n",
        "        search = user_input\n",
        "\n",
        "        # Alternatively simply use search_client.search(q, top=3) if not using semantic search\n",
        "        filter = \"category ne '{}'\".format(exclude_category.replace(\"'\", \"''\")) if exclude_category else None\n",
        "        r = search_client.search(search,\n",
        "                            filter=filter,\n",
        "                            query_type=\"semantic\",\n",
        "                            query_language=\"en-us\",\n",
        "                            query_speller=\"lexicon\",\n",
        "                            semantic_configuration_name=semantic_configuration_name,\n",
        "                            top=top)\n",
        "\n",
        "        search_results = [doc for doc in r]\n",
        "        results = [doc['content'][:2000].replace(\"\\n\", \"\").replace(\"\\r\", \"\") for doc in search_results]\n",
        "\n",
        "    prompt = prompt_prefix.format(sources=results) + prompt_history + user_input + turn_suffix\n",
        "\n",
        "    try:\n",
        "        completion = \"\"\n",
        "        completion = generate_completion(engine=openai_gpt_deployment, prompt=prompt, temperature=0.7, max_tokens=1024, stop=[\"<|im_end|>\", \"<|im_start|>\"])\n",
        "        completion = completion.replace('\"', \"\").replace(\",\", \"\").replace(\"'\", \"\")\n",
        "    except Exception as e:\n",
        "        has_error = True\n",
        "        error_string = e\n",
        "\n",
        "    # Build the search output\n",
        "    output = []\n",
        "    values = {}\n",
        "\n",
        "    for i, doc in enumerate(search_results):\n",
        "        values['\"'+doc['file']+'\"'] = ['\"'+doc['id']+'\"', '\"'+str(doc['@search.score'])+'\"', '\"'+str(doc['@search.reranker_score'])+'\"', '\"'+str(i)+'\"', '\"'+completion+'\"']\n",
        "    output.append(values)\n",
        "\n",
        "\n",
        "    if has_error == False:\n",
        "        prompt_history += user_input + turn_suffix + completion + \"\\n<|im_end|>\" + turn_prefix\n",
        "        history.append(\"user: \" + user_input)\n",
        "        history.append(\"assistant: \" + completion)\n",
        "\n",
        "    if has_error == True:\n",
        "        return str(error_string)\n",
        "    else:\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "udf_evaluateSemanticSearchRAG = udf(evaluateSemanticSearchRAG, StringType())\n",
        "df_questions = df_questions.withColumn(\"SemanticSearchRAG\", udf_evaluateSemanticSearchRAG(col(\"question\")))\n",
        "df_questions.write.saveAsTable(\"2eval_step4\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Vector search - use requests to avoid dev SDK version "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Test the vector search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-29T09:07:01.9573501Z",
              "execution_start_time": "2023-06-29T09:07:00.8429588Z",
              "livy_statement_state": "available",
              "parent_msg_id": "fbbad12f-e76f-48d9-87ae-3a91b67a2cf2",
              "queued_time": "2023-06-29T09:07:00.6858803Z",
              "session_id": "92",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 29
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 92, 29, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'rugby008.txt'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "searchFields = 'content, file'\n",
        "selectFields = 'content, file'\n",
        "scoringProfile = None\n",
        "\n",
        "serviceName = search_service_name\n",
        "service_endpoint = (f\"https://{serviceName}.search.windows.net\")\n",
        "indexName = search_index\n",
        "apiKey = search_admin_key\n",
        "apiVersion = \"2023-07-01-Preview\"\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "def generate_embeddings(text):\n",
        "    response = openai.Embedding.create(\n",
        "        input=text, deployment_id=openai_embeddings_deployment_id)\n",
        "    embeddings = response['data'][0]['embedding']\n",
        "    return embeddings\n",
        "\n",
        "def getServiceUrl():\n",
        "    return 'https://' + serviceName + '.search.windows.net'\n",
        "\n",
        "def getMethod(servicePath):\n",
        "    headers = {'Content-type': 'application/json', 'api-key': apiKey}\n",
        "    r = requests.get(getServiceUrl() + servicePath, headers=headers)\n",
        "    return r\n",
        "\n",
        "def postMethod(servicePath, body):\n",
        "    headers = {'Content-type': 'application/json', 'api-key': apiKey}\n",
        "    r = requests.post(getServiceUrl() + servicePath, headers=headers, data=body)\n",
        "    return r\n",
        "\n",
        "def simpleHybridSearch(servicePath, query, top):\n",
        "    show = False\n",
        "    query_embedding = generate_embeddings(query)\n",
        "\n",
        "    values = ObjDict()\n",
        "    values.values = {}\n",
        "    vector = {}\n",
        "\n",
        "    vector['value'] = query_embedding\n",
        "    vector['fields'] = \"contentVector\"\n",
        "    vector['k'] = top\n",
        "    values.values['vector'] = vector\n",
        "    values.values['search'] = query\n",
        "    values.values['top'] = top\n",
        "\n",
        "    body_json = json.dumps(values.values)\n",
        "    servicePath = '/indexes/' + indexName + '/docs/search?api-version=%s' % (apiVersion)\n",
        "    r = postMethod(servicePath, body_json)\n",
        "    return r\n",
        "        \n",
        "def submitQuery(query, fields=None, select=None, scoring=None, top=top, fuzzy=False, method=\"GET\"):\n",
        "    servicePath = '/indexes/' + indexName + '/docs?api-version=%s&search=%s&$top=%d' % \\\n",
        "        (apiVersion, query, top)\n",
        "    if fields is not None:\n",
        "        servicePath += '&searchFields=%s' % fields\n",
        "    if select is not None:\n",
        "        servicePath += '&$select=%s' % select\n",
        "    if scoring is not None:\n",
        "        servicePath += '&scoringProfile=%s' % scoring\n",
        "    if fuzzy:\n",
        "        servicePath += '&queryType=full'\n",
        "\n",
        "    if method == \"GET\":\n",
        "        r = getMethod(servicePath)\n",
        "    elif method == \"simpleHybridSearch\":\n",
        "        r = simpleHybridSearch(servicePath, query, top)\n",
        "    elif method == \"semanticHybridSearch\":\n",
        "        r = semanticHybridSearch(servicePath, query, top)\n",
        "    if r.status_code != 200:\n",
        "        print('Failed to retrieve search results')\n",
        "        print(query, r, r.text)\n",
        "        return {}\n",
        "    docs = json.loads(r.text)['value']\n",
        "    return docs\n",
        "\n",
        "results = submitQuery(\"Who broke their thumb?\", fields=searchFields, select=selectFields, scoring=scoringProfile, top=10, fuzzy=False, method=\"simpleHybridSearch\")\n",
        "print(results[0]['file'])\n",
        "assert results[0]['file'] == \"rugby008.txt\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step 5 Simple vector search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-27T15:28:24.5053481Z",
              "execution_start_time": "2023-06-27T14:39:56.6900212Z",
              "livy_statement_state": "available",
              "parent_msg_id": "04080c5c-d051-4e02-b376-aeac4bdd411a",
              "queued_time": "2023-06-27T14:39:56.5325075Z",
              "session_id": "90",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 29
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 90, 29, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def simpleHybridSearch(user_input):\n",
        "\n",
        "    serviceName = search_service_name\n",
        "    service_endpoint = (f\"https://{serviceName}.search.windows.net\")\n",
        "    indexName = search_index\n",
        "    apiKey = search_admin_key\n",
        "    apiVersion = \"2023-07-01-Preview\"\n",
        "\n",
        "    openai.api_key = openai_api_key\n",
        "    openai.api_type = openai_api_type\n",
        "    openai.api_base = f\"https://{openai_service}.openai.azure.com\"\n",
        "    openai.api_version = openai_api_version\n",
        "\n",
        "    searchFields = 'content, file'\n",
        "    selectFields = 'content, file'\n",
        "    scoringProfile = None\n",
        "\n",
        "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "    def generate_completion(engine, prompt, temperature, max_tokens, stop):\n",
        "        completion = openai.Completion.create(\n",
        "                        engine=engine,\n",
        "                        prompt=prompt,\n",
        "                        temperature=temperature,\n",
        "                        max_tokens=max_tokens,\n",
        "                        stop=stop)\n",
        "        return completion.choices[0].text\n",
        "\n",
        "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "    def generate_embeddings(text):\n",
        "        response = openai.Embedding.create(\n",
        "            input=text, deployment_id=openai_embeddings_deployment_id)\n",
        "        embeddings = response['data'][0]['embedding']\n",
        "        return embeddings\n",
        "\n",
        "    def getServiceUrl():\n",
        "        return 'https://' + serviceName + '.search.windows.net'\n",
        "\n",
        "    def getMethod(servicePath):\n",
        "        headers = {'Content-type': 'application/json', 'api-key': apiKey}\n",
        "        r = requests.get(getServiceUrl() + servicePath, headers=headers)\n",
        "        return r\n",
        "\n",
        "    def postMethod(servicePath, body):\n",
        "        headers = {'Content-type': 'application/json', 'api-key': apiKey}\n",
        "        r = requests.post(getServiceUrl() + servicePath, headers=headers, data=body)\n",
        "        return r\n",
        "\n",
        "    def simpleHybridSearch(servicePath, query, top):\n",
        "        show = False\n",
        "        query_embedding = generate_embeddings(query)\n",
        "\n",
        "        values = ObjDict()\n",
        "        values.values = {}\n",
        "        vector = {}\n",
        "\n",
        "        vector['value'] = query_embedding\n",
        "        vector['fields'] = \"contentVector\"\n",
        "        vector['k'] = top\n",
        "        values.values['vector'] = vector\n",
        "        values.values['search'] = query\n",
        "        values.values['top'] = top\n",
        "\n",
        "        body_json = json.dumps(values.values)\n",
        "        servicePath = '/indexes/' + indexName + '/docs/search?api-version=%s' % (apiVersion)\n",
        "        r = postMethod(servicePath, body_json)\n",
        "        return r\n",
        "            \n",
        "    def submitQuery(query, fields=None, select=None, scoring=None, top=top, fuzzy=False, method=\"GET\"):\n",
        "        servicePath = '/indexes/' + indexName + '/docs?api-version=%s&search=%s&$top=%d' % \\\n",
        "            (apiVersion, query, top)\n",
        "        if fields is not None:\n",
        "            servicePath += '&searchFields=%s' % fields\n",
        "        if select is not None:\n",
        "            servicePath += '&$select=%s' % select\n",
        "        if scoring is not None:\n",
        "            servicePath += '&scoringProfile=%s' % scoring\n",
        "        if fuzzy:\n",
        "            servicePath += '&queryType=full'\n",
        "\n",
        "        if method == \"GET\":\n",
        "            r = getMethod(servicePath)\n",
        "        elif method == \"simpleHybridSearch\":\n",
        "            r = simpleHybridSearch(servicePath, query, top)\n",
        "        elif method == \"semanticHybridSearch\":\n",
        "            r = semanticHybridSearch(servicePath, query, top)\n",
        "        if r.status_code != 200:\n",
        "            print('Failed to retrieve search results')\n",
        "            print(query, r, r.text)\n",
        "            return {}\n",
        "        docs = json.loads(r.text)['value']\n",
        "        return docs\n",
        "  \n",
        "    search_results = submitQuery(user_input, fields=searchFields, select=selectFields, scoring=scoringProfile, top=top, fuzzy=False, method=\"simpleHybridSearch\")\n",
        "\n",
        "    # Build the search output\n",
        "    output = []\n",
        "    values = {}\n",
        "\n",
        "    for i, doc in enumerate(search_results):\n",
        "        values['\"'+doc['file']+'\"'] = ['\"'+doc['id']+'\"', '\"'+str(doc['@search.score'])+'\"', '\"'+\"None\"+'\"', '\"'+str(i)+'\"']\n",
        "    output.append(values)\n",
        "\n",
        "    return output\n",
        "\n",
        "udf_simpleHybridSearch = udf(simpleHybridSearch, ArrayType(StringType()))\n",
        "df_questions = df_questions.withColumn(\"SimpleHybridSearch\", udf_simpleHybridSearch(col(\"question\")))\n",
        "df_questions.write.saveAsTable(\"2eval_step5\")\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step 6 Semantic Hybrid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-27T16:41:33.4096747Z",
              "execution_start_time": "2023-06-27T15:42:16.589968Z",
              "livy_statement_state": "available",
              "parent_msg_id": "0402528f-8c5a-4767-99b2-20ca3cc4fe51",
              "queued_time": "2023-06-27T15:42:16.3286401Z",
              "session_id": "90",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 31
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 90, 31, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def semanticHybridSearch(user_input):\n",
        "\n",
        "    serviceName = search_service_name\n",
        "    service_endpoint = (f\"https://{serviceName}.search.windows.net\")\n",
        "    indexName = search_index\n",
        "    apiKey = search_admin_key\n",
        "    apiVersion = \"2023-07-01-Preview\"\n",
        "\n",
        "    openai.api_key = openai_api_key\n",
        "    openai.api_type = openai_api_type\n",
        "    openai.api_base = f\"https://{openai_service}.openai.azure.com\"\n",
        "    openai.api_version = openai_api_version\n",
        "\n",
        "    searchFields = 'content, file'\n",
        "    selectFields = 'content, file'\n",
        "    scoringProfile = None\n",
        "\n",
        "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "    def generate_completion(engine, prompt, temperature, max_tokens, stop):\n",
        "        completion = openai.Completion.create(\n",
        "                        engine=engine,\n",
        "                        prompt=prompt,\n",
        "                        temperature=temperature,\n",
        "                        max_tokens=max_tokens,\n",
        "                        stop=stop)\n",
        "        return completion.choices[0].text\n",
        "\n",
        "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "    def generate_embeddings(text):\n",
        "        response = openai.Embedding.create(\n",
        "            input=text, deployment_id=openai_embeddings_deployment_id)\n",
        "        embeddings = response['data'][0]['embedding']\n",
        "        return embeddings\n",
        "\n",
        "    def getServiceUrl():\n",
        "        return 'https://' + serviceName + '.search.windows.net'\n",
        "\n",
        "    def getMethod(servicePath):\n",
        "        headers = {'Content-type': 'application/json', 'api-key': apiKey}\n",
        "        r = requests.get(getServiceUrl() + servicePath, headers=headers)\n",
        "        return r\n",
        "\n",
        "    def postMethod(servicePath, body):\n",
        "        headers = {'Content-type': 'application/json', 'api-key': apiKey}\n",
        "        r = requests.post(getServiceUrl() + servicePath, headers=headers, data=body)\n",
        "        return r\n",
        "\n",
        "    def semanticHybridSearch(servicePath, query, ntop):\n",
        "\n",
        "        query_embedding = generate_embeddings(query)\n",
        "\n",
        "        values = ObjDict()\n",
        "        values.values = {}\n",
        "        vector = {}\n",
        "\n",
        "        vector['value'] = query_embedding\n",
        "        vector['fields'] = \"contentVector\"\n",
        "        vector['k'] = top\n",
        "        values.values['vector'] = vector\n",
        "        values.values['search'] = query\n",
        "        values.values['queryType'] = \"semantic\"\n",
        "        values.values['semanticConfiguration'] = \"bbc-semantic-config2\"\n",
        "        values.values['queryLanguage'] = \"en-us\"\n",
        "        values.values['captions'] = \"extractive\"\n",
        "        values.values['answers'] = \"extractive\"\n",
        "        values.values['top'] = top\n",
        "\n",
        "        body_json = json.dumps(values.values)\n",
        "        servicePath = '/indexes/' + indexName + '/docs/search?api-version=%s' % (apiVersion)\n",
        "        r = postMethod(servicePath, body_json)\n",
        "        return r\n",
        "            \n",
        "    def submitQuery(query, fields=None, select=None, scoring=None, top=top, fuzzy=False, method=\"GET\"):\n",
        "        servicePath = '/indexes/' + indexName + '/docs?api-version=%s&search=%s&$top=%d' % \\\n",
        "            (apiVersion, query, top)\n",
        "        if fields is not None:\n",
        "            servicePath += '&searchFields=%s' % fields\n",
        "        if select is not None:\n",
        "            servicePath += '&$select=%s' % select\n",
        "        if scoring is not None:\n",
        "            servicePath += '&scoringProfile=%s' % scoring\n",
        "        if fuzzy:\n",
        "            servicePath += '&queryType=full'\n",
        "\n",
        "        if method == \"GET\":\n",
        "            r = getMethod(servicePath)\n",
        "        elif method == \"simpleHybridSearch\":\n",
        "            r = simpleHybridSearch(servicePath, query, top)\n",
        "        elif method == \"semanticHybridSearch\":\n",
        "            r = semanticHybridSearch(servicePath, query, top)\n",
        "        if r.status_code != 200:\n",
        "            print('Failed to retrieve search results')\n",
        "            print(query, r, r.text)\n",
        "            return {}\n",
        "        docs = json.loads(r.text)['value']\n",
        "        return docs\n",
        "  \n",
        "    search_results = submitQuery(user_input, fields=searchFields, select=selectFields, scoring=scoringProfile, top=top, fuzzy=False, method=\"semanticHybridSearch\")\n",
        "\n",
        "    # Build the search output\n",
        "    output = []\n",
        "    values = {}\n",
        "\n",
        "    for i, doc in enumerate(search_results):\n",
        "        values['\"'+doc['file']+'\"'] = ['\"'+doc['id']+'\"', '\"'+str(doc['@search.score'])+'\"', '\"'+str(doc['@search.rerankerScore'])+'\"', '\"'+str(i)+'\"']\n",
        "    output.append(values)\n",
        "\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "udf_semanticHybridSearch = udf(semanticHybridSearch, ArrayType(StringType()))\n",
        "df_questions = df_questions.withColumn(\"SemanticHybridSearch\", udf_semanticHybridSearch(col(\"question\")))\n",
        "\n",
        "df_questions.write.saveAsTable(\"2eval_step6\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step 7 Simple Hybrid Search RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-27T18:11:08.5804214Z",
              "execution_start_time": "2023-06-27T16:45:49.676368Z",
              "livy_statement_state": "available",
              "parent_msg_id": "74fa3b23-564b-436e-8ebe-61678dbf8682",
              "queued_time": "2023-06-27T16:45:49.5195916Z",
              "session_id": "90",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 34
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 90, 34, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#7\n",
        "\n",
        "def evaluateHybridSimpleSearchRAG(user_input):\n",
        "    \n",
        "    serviceName = search_service_name\n",
        "    service_endpoint = (f\"https://{serviceName}.search.windows.net\")\n",
        "    indexName = search_index\n",
        "    apiKey = search_admin_key\n",
        "    apiVersion = \"2023-07-01-Preview\"\n",
        "\n",
        "    openai.api_key = openai_api_key\n",
        "    openai.api_type = openai_api_type\n",
        "    openai.api_base = f\"https://{openai_service}.openai.azure.com\"\n",
        "    openai.api_version = openai_api_version\n",
        "\n",
        "    searchFields = 'content, file'\n",
        "    selectFields = 'content, file'\n",
        "    scoringProfile = None\n",
        "\n",
        "\n",
        "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "    def generate_completion(engine, prompt, temperature, max_tokens, stop):\n",
        "        completion = openai.Completion.create(\n",
        "                        engine=engine,\n",
        "                        prompt=prompt,\n",
        "                        temperature=temperature,\n",
        "                        max_tokens=max_tokens,\n",
        "                        stop=stop)\n",
        "        return completion.choices[0].text\n",
        "\n",
        "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "    def generate_embeddings(text):\n",
        "        response = openai.Embedding.create(\n",
        "            input=text, deployment_id=openai_embeddings_deployment_id)\n",
        "        embeddings = response['data'][0]['embedding']\n",
        "        return embeddings\n",
        "\n",
        "    def getServiceUrl():\n",
        "        return 'https://' + serviceName + '.search.windows.net'\n",
        "\n",
        "    def getMethod(servicePath):\n",
        "        headers = {'Content-type': 'application/json', 'api-key': apiKey}\n",
        "        r = requests.get(getServiceUrl() + servicePath, headers=headers)\n",
        "        return r\n",
        "\n",
        "    def postMethod(servicePath, body):\n",
        "        headers = {'Content-type': 'application/json', 'api-key': apiKey}\n",
        "        r = requests.post(getServiceUrl() + servicePath, headers=headers, data=body)\n",
        "        return r\n",
        "\n",
        "    def simpleHybridSearch(servicePath, query, top):\n",
        "        \n",
        "        query_embedding = generate_embeddings(query)\n",
        "\n",
        "        values = ObjDict()\n",
        "        values.values = {}\n",
        "        vector = {}\n",
        "\n",
        "        vector['value'] = query_embedding\n",
        "        vector['fields'] = \"contentVector\"\n",
        "        vector['k'] = top\n",
        "        values.values['vector'] = vector\n",
        "        values.values['search'] = query\n",
        "        values.values['top'] = top\n",
        "        body_json = json.dumps(values.values)\n",
        "        servicePath = '/indexes/' + indexName + '/docs/search?api-version=%s' % (apiVersion)\n",
        "        r = postMethod(servicePath, body_json)\n",
        "\n",
        "        return r\n",
        "\n",
        "\n",
        "    def submitQuery(query, fields=None, select=None, scoring=None, top=top, fuzzy=False, method=\"GET\"):\n",
        "            servicePath = '/indexes/' + indexName + '/docs?api-version=%s&search=%s&$top=%d' % \\\n",
        "                (apiVersion, query, top)\n",
        "            if fields is not None:\n",
        "                servicePath += '&searchFields=%s' % fields\n",
        "            if select is not None:\n",
        "                servicePath += '&$select=%s' % select\n",
        "            if scoring is not None:\n",
        "                servicePath += '&scoringProfile=%s' % scoring\n",
        "            if fuzzy:\n",
        "                servicePath += '&queryType=full'\n",
        "\n",
        "            if method == \"GET\":\n",
        "                r = getMethod(servicePath)\n",
        "            elif method == \"simpleHybridSearch\":\n",
        "                r = simpleHybridSearch(servicePath, query, top)\n",
        "            elif method == \"semanticHybridSearch\":\n",
        "                r = semanticHybridSearch(servicePath, query, top)\n",
        "            if r.status_code != 200:\n",
        "                print('Failed to retrieve search results')\n",
        "                print(query, r, r.text)\n",
        "                return {}\n",
        "            docs = json.loads(r.text)['value']\n",
        "            return docs\n",
        "\n",
        "\n",
        "    error_string = \"\"\n",
        "    has_error = False\n",
        "\n",
        "    openai.api_key = openai_api_key\n",
        "    openai.api_type = openai_api_type\n",
        "    openai.api_base = f\"https://{openai_service}.openai.azure.com\"\n",
        "    openai.api_version = openai_api_version\n",
        "\n",
        "    prompt_prefix = \"\"\"<|im_start|>system\n",
        "    Let's work this out it a step by step to be sure we have the right answer\n",
        "\n",
        "    Sources:\n",
        "    {sources}\n",
        "    \n",
        "\n",
        "    <|im_end|>\"\"\"\n",
        "\n",
        "    turn_prefix = \"\"\"\n",
        "    <|im_start|>user\n",
        "    \"\"\"\n",
        "\n",
        "    turn_suffix = \"\"\"\n",
        "    <|im_end|>\n",
        "    <|im_start|>assistant\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_history = turn_prefix\n",
        "\n",
        "    history = []\n",
        "\n",
        "    summary_prompt_template = \"\"\"Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base. Generate a search query based on the conversation and the new question. Source names are not good search terms to include in the search query.\n",
        "\n",
        "    Summary:\n",
        "    {summary}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Search query:\n",
        "    \"\"\"\n",
        "\n",
        "    content = \"\"\n",
        "\n",
        "    # Exclude category, to simulate scenarios where there's a set of docs you can't see\n",
        "    exclude_category = None\n",
        "\n",
        "    if len(history) > 0:\n",
        "\n",
        "        try:\n",
        "            search = generate_completion(engine=openai_gpt_deployment, prompt=summary_prompt_template.format(summary=\"\\n\".join(history), question=user_input), temperature=0.9, max_tokens=320, stop=[\"\\n\"])\n",
        "        except Exception as e:\n",
        "            has_error = True\n",
        "            error_string = e\n",
        "    else:\n",
        "        search = user_input\n",
        "\n",
        "        # Alternatively simply use search_client.search(q, top=3) if not using semantic search\n",
        "        filter = \"category ne '{}'\".format(exclude_category.replace(\"'\", \"''\")) if exclude_category else None\n",
        "        r = submitQuery(user_input, fields=searchFields, select=selectFields, scoring=scoringProfile, top=top, fuzzy=False, method=\"simpleHybridSearch\")\n",
        "        search_results = [doc for doc in r]\n",
        "        results = [doc['content'][:2000].replace(\"\\n\", \"\").replace(\"\\r\", \"\") for doc in search_results]\n",
        "\n",
        "\n",
        "    prompt = prompt_prefix.format(sources=results) + prompt_history + user_input + turn_suffix\n",
        "\n",
        "    try:\n",
        "        completion = \"\"\n",
        "        completion = generate_completion(engine=openai_gpt_deployment, prompt=prompt, temperature=0.7, max_tokens=1024, stop=[\"<|im_end|>\", \"<|im_start|>\"])\n",
        "        completion = completion.replace('\"', \"\").replace(\",\", \"\").replace(\"'\", \"\")\n",
        "    except Exception as e:\n",
        "        has_error = True\n",
        "        error_string = e\n",
        "\n",
        "\n",
        "    if has_error == False:\n",
        "        prompt_history += user_input + turn_suffix + completion + \"\\n<|im_end|>\" + turn_prefix\n",
        "        history.append(\"user: \" + user_input)\n",
        "        history.append(\"assistant: \" + completion)\n",
        "\n",
        "    # Build the search output\n",
        "    output = []\n",
        "    values = {}\n",
        "\n",
        "    for i, doc in enumerate(search_results):\n",
        "        values['\"'+doc['file']+'\"'] = ['\"'+doc['id']+'\"', '\"'+str(doc['@search.score'])+'\"', '\"'+\"None\"+'\"', '\"'+str(i)+'\"', '\"'+completion+'\"']\n",
        "    output.append(values)\n",
        "\n",
        "    if has_error == True:\n",
        "        return str(error_string)\n",
        "    else:\n",
        "        return output\n",
        "\n",
        "udf_evaluateHybridSimpleSearchRAG = udf(evaluateHybridSimpleSearchRAG, ArrayType(StringType()))\n",
        "df_questions = df_questions.withColumn(\"SimpleHybridSearchRAG\", udf_evaluateHybridSimpleSearchRAG(col(\"question\")))\n",
        "df_questions.write.saveAsTable(\"2eval_step7\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Step 8 Semantic Hybrid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-27T20:06:56.4317468Z",
              "execution_start_time": "2023-06-27T18:15:43.5109325Z",
              "livy_statement_state": "available",
              "parent_msg_id": "22959da5-f87e-4ab6-ae82-b47f3c2d0b59",
              "queued_time": "2023-06-27T18:15:43.3503096Z",
              "session_id": "90",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 37
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 90, 37, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#8\n",
        "\n",
        "def evaluateHybridSemanticSearchRAG(user_input):\n",
        "    #sleep(2)\n",
        "\n",
        "    serviceName = search_service_name\n",
        "    service_endpoint = (f\"https://{serviceName}.search.windows.net\")\n",
        "    indexName = search_index\n",
        "    apiKey = search_admin_key\n",
        "    apiVersion = \"2023-07-01-Preview\"\n",
        "    searchFields = 'content, file'\n",
        "    selectFields = 'content, file'\n",
        "    scoringProfile = None\n",
        "\n",
        "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "    def generate_completion(engine, prompt, temperature, max_tokens, stop):\n",
        "        completion = openai.Completion.create(\n",
        "                        engine=engine,\n",
        "                        prompt=prompt,\n",
        "                        temperature=temperature,\n",
        "                        max_tokens=max_tokens,\n",
        "                        stop=stop)\n",
        "        return completion.choices[0].text\n",
        "\n",
        "    @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "    def generate_embeddings(text):\n",
        "        response = openai.Embedding.create(\n",
        "            input=text, deployment_id=openai_embeddings_deployment_id)\n",
        "        embeddings = response['data'][0]['embedding']\n",
        "        return embeddings\n",
        "\n",
        "    def getServiceUrl():\n",
        "        return 'https://' + serviceName + '.search.windows.net'\n",
        "\n",
        "    def getMethod(servicePath):\n",
        "        headers = {'Content-type': 'application/json', 'api-key': apiKey}\n",
        "        r = requests.get(getServiceUrl() + servicePath, headers=headers)\n",
        "        return r\n",
        "\n",
        "    def postMethod(servicePath, body):\n",
        "        headers = {'Content-type': 'application/json', 'api-key': apiKey}\n",
        "        r = requests.post(getServiceUrl() + servicePath, headers=headers, data=body)\n",
        "        return r\n",
        "\n",
        "    def semanticHybridSearch(servicePath, query, ntop):\n",
        "\n",
        "        query_embedding = generate_embeddings(query)\n",
        "\n",
        "        values = ObjDict()\n",
        "        values.values = {}\n",
        "        vector = {}\n",
        "\n",
        "        vector['value'] = query_embedding\n",
        "        vector['fields'] = \"contentVector\"\n",
        "        vector['k'] = top\n",
        "        values.values['vector'] = vector\n",
        "        values.values['search'] = query\n",
        "        values.values['queryType'] = \"semantic\"\n",
        "        values.values['semanticConfiguration'] = \"bbc-semantic-config2\"\n",
        "        values.values['queryLanguage'] = \"en-us\"\n",
        "        values.values['captions'] = \"extractive\"\n",
        "        values.values['answers'] = \"extractive\"\n",
        "        values.values['top'] = top\n",
        "\n",
        "        body_json = json.dumps(values.values)\n",
        "        servicePath = '/indexes/' + indexName + '/docs/search?api-version=%s' % (apiVersion)\n",
        "        r = postMethod(servicePath, body_json)\n",
        "        return r\n",
        "\n",
        "    \n",
        "    def submitQuery(query, fields=None, select=None, scoring=None, top=top, fuzzy=False, method=\"GET\"):\n",
        "            servicePath = '/indexes/' + indexName + '/docs?api-version=%s&search=%s&$top=%d' % \\\n",
        "                (apiVersion, query, top)\n",
        "            if fields is not None:\n",
        "                servicePath += '&searchFields=%s' % fields\n",
        "            if select is not None:\n",
        "                servicePath += '&$select=%s' % select\n",
        "            if scoring is not None:\n",
        "                servicePath += '&scoringProfile=%s' % scoring\n",
        "            if fuzzy:\n",
        "                servicePath += '&queryType=full'\n",
        "\n",
        "            if method == \"GET\":\n",
        "                r = getMethod(servicePath)\n",
        "            elif method == \"simpleHybridSearch\":\n",
        "                r = simpleHybridSearch(servicePath, query, top)\n",
        "            elif method == \"semanticHybridSearch\":\n",
        "                r = semanticHybridSearch(servicePath, query, top)\n",
        "            if r.status_code != 200:\n",
        "                print('Failed to retrieve search results')\n",
        "                print(query, r, r.text)\n",
        "                return {}\n",
        "            docs = json.loads(r.text)['value']\n",
        "            return docs\n",
        "\n",
        "\n",
        "    error_string = \"\"\n",
        "    has_error = False\n",
        "\n",
        "\n",
        "    openai.api_key = openai_api_key\n",
        "    openai.api_type = openai_api_type\n",
        "    openai.api_base = f\"https://{openai_service}.openai.azure.com\"\n",
        "    openai.api_version = openai_api_version\n",
        "\n",
        "    prompt_prefix = \"\"\"<|im_start|>system\n",
        "    Let's work this out it a step by step to be sure we have the right answer\n",
        "\n",
        "    Sources:\n",
        "    {sources}\n",
        "    \n",
        "    <|im_end|>\"\"\"\n",
        "\n",
        "    turn_prefix = \"\"\"\n",
        "    <|im_start|>user\n",
        "    \"\"\"\n",
        "\n",
        "    turn_suffix = \"\"\"\n",
        "    <|im_end|>\n",
        "    <|im_start|>assistant\n",
        "    \"\"\"\n",
        "\n",
        "    prompt_history = turn_prefix\n",
        "\n",
        "    history = []\n",
        "\n",
        "    summary_prompt_template = \"\"\"Below is a summary of the conversation so far, and a new question asked by the user that needs to be answered by searching in a knowledge base. Generate a search query based on the conversation and the new question. Source names are not good search terms to include in the search query.\n",
        "\n",
        "    Summary:\n",
        "    {summary}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Search query:\n",
        "    \"\"\"\n",
        "\n",
        "    content = \"\"\n",
        "\n",
        "    # Exclude category, to simulate scenarios where there's a set of docs you can't see\n",
        "    exclude_category = None\n",
        "\n",
        "    if len(history) > 0:\n",
        "\n",
        "        try:\n",
        "            search = generate_completion(engine=openai_gpt_deployment, prompt=summary_prompt_template.format(summary=\"\\n\".join(history), question=user_input), temperature=0.9, max_tokens=320, stop=[\"\\n\"])\n",
        "        except Exception as e:\n",
        "            has_error = True\n",
        "            error_string = e\n",
        "\n",
        "        print(f\"search {search}\")\n",
        "    else:\n",
        "        search = user_input\n",
        "\n",
        "        # Alternatively simply use search_client.search(q, top=3) if not using semantic search\n",
        "        filter = \"category ne '{}'\".format(exclude_category.replace(\"'\", \"''\")) if exclude_category else None\n",
        "        r = submitQuery(user_input, fields=searchFields, select=selectFields, scoring=scoringProfile, top=top, fuzzy=False, method=\"semanticHybridSearch\")\n",
        "        search_results = [doc for doc in r]\n",
        "        results = [doc['content'][:2000].replace(\"\\n\", \"\").replace(\"\\r\", \"\") for doc in search_results]\n",
        "\n",
        "\n",
        "    prompt = prompt_prefix.format(sources=results) + prompt_history + user_input + turn_suffix\n",
        "\n",
        "    try:\n",
        "        completion = \"\"\n",
        "        completion = generate_completion(engine=openai_gpt_deployment, prompt=prompt, temperature=0.7, max_tokens=1024, stop=[\"<|im_end|>\", \"<|im_start|>\"])\n",
        "        completion = completion.replace('\"', \"\").replace(\",\", \"\").replace(\"'\", \"\")\n",
        "    except Exception as e:\n",
        "        has_error = True\n",
        "        error_string = e\n",
        "\n",
        "    if has_error == False:\n",
        "        prompt_history += user_input + turn_suffix + completion + \"\\n<|im_end|>\" + turn_prefix\n",
        "        history.append(\"user: \" + user_input)\n",
        "        history.append(\"assistant: \" + completion)\n",
        "\n",
        "    # Build the search output\n",
        "    output = []\n",
        "    values = {}\n",
        "    \n",
        "    for i, doc in enumerate(search_results):\n",
        "        values['\"'+doc['file']+'\"'] = ['\"'+doc['id']+'\"', '\"'+str(doc['@search.score'])+'\"', '\"'+str(doc['@search.rerankerScore'])+'\"', '\"'+str(i)+'\"','\"'+completion+'\"']\n",
        "    output.append(values)\n",
        "\n",
        "    if has_error == True:\n",
        "        return str(error_string)\n",
        "    else:\n",
        "        return output\n",
        "\n",
        "udf_evaluateHybridSemanticSearchRAG = udf(evaluateHybridSemanticSearchRAG, ArrayType(StringType()))\n",
        "df_questions = df_questions.withColumn(\"SemanticHybridSearchRAG\", udf_evaluateHybridSemanticSearchRAG(col(\"question\")))\n",
        "\n",
        "df_questions.write.saveAsTable(\"2eval_step8\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-27T21:56:00.4996591Z",
              "execution_start_time": "2023-06-27T20:08:16.7934942Z",
              "livy_statement_state": "available",
              "parent_msg_id": "7eb83afc-5cdb-4798-b63c-0d541b071ac5",
              "queued_time": "2023-06-27T20:08:16.6328508Z",
              "session_id": "90",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 38
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 90, 38, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "dfq = df_questions.toPandas()\n",
        "dfq.to_csv(\"abfss://share@datadiscoverypipeline2.dfs.core.windows.net/bbcsports/csv/dfq_questions2.csv\", sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-24T14:49:06.9417169Z",
              "execution_start_time": "2023-06-24T14:48:03.9122574Z",
              "livy_statement_state": "available",
              "parent_msg_id": "3ce8961d-dd1a-4e07-96fd-cb5dd1beb75e",
              "queued_time": "2023-06-24T14:48:03.7584321Z",
              "session_id": "86",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 3
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 86, 3, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-24 14:48:10.873767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c2cc350-5086-4388-9941-017b24eba9e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "941d9014-fa74-4c35-ae21-c97ccb0ac9f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5aebfeeb-ad03-400c-86ea-9dcbe23c0112",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01ea560b-a452-49cb-af9c-24041080cc55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5931c871-2783-4c92-8adc-76c85fe066a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5fd22ea-0775-4a80-bbd2-b7cff25309cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4f885e5-a061-40b3-89fb-cc5b40452f6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49e035b4-87eb-4706-889e-1564e4d6d505",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-24T14:49:07.4046865Z",
              "execution_start_time": "2023-06-24T14:49:07.190683Z",
              "livy_statement_state": "available",
              "parent_msg_id": "d90c74ec-748a-464a-8e7e-51e4b54cf085",
              "queued_time": "2023-06-24T14:48:59.3812302Z",
              "session_id": "86",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 4
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 86, 4, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-24T14:49:08.9496581Z",
              "execution_start_time": "2023-06-24T14:49:08.7862277Z",
              "livy_statement_state": "available",
              "parent_msg_id": "ed286005-db3b-4874-8fbb-4348b681c359",
              "queued_time": "2023-06-24T14:49:08.6317714Z",
              "session_id": "86",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 5
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 86, 5, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-24T14:49:10.1726052Z",
              "execution_start_time": "2023-06-24T14:49:10.001334Z",
              "livy_statement_state": "available",
              "parent_msg_id": "9bdd95dc-1178-4c8d-a902-10acf4dffbe3",
              "queued_time": "2023-06-24T14:49:09.8505524Z",
              "session_id": "86",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 6
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 86, 6, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-06-24T14:49:11.6078244Z",
              "execution_start_time": "2023-06-24T14:49:11.4157949Z",
              "livy_statement_state": "available",
              "parent_msg_id": "5c9822e2-ad67-489c-b44b-0a97ad631ec6",
              "queued_time": "2023-06-24T14:49:11.2441431Z",
              "session_id": "86",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "DataDiscovery",
              "state": "finished",
              "statement_id": 7
            },
            "text/plain": [
              "StatementMeta(DataDiscovery, 86, 7, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from evaluate import load\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "exact_match_metric = load(\"exact_match\")\n",
        "trec_eval = load(\"trec_eval\")\n",
        "\n",
        "# We use entailment to do a simple check on the OpenAI answer\n",
        "global nli_model\n",
        "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
        "\n",
        "global nli_tokenizer\n",
        "nli_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Test the entailment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "hyp = \"I like ice cream\"\n",
        "\n",
        "# roBERTA Entailment\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# run through model pre-trained on MNLI\n",
        "x = nli_tokenizer.encode(\"Do you like ice cream?\", hyp, return_tensors='pt',\n",
        "                        truncation_strategy='only_first')\n",
        "logits = nli_model(x.to(device))[0]\n",
        "\n",
        "# we throw away \"neutral\" (dim 1) and take the probability of\n",
        "# \"entailment\" (2) as the probability of the label being true\n",
        "entail_contradiction_logits = logits[:, [0, 2]]\n",
        "probs = entail_contradiction_logits.softmax(dim=1)\n",
        "prob_label_is_true = probs[:, 1]\n",
        "assert prob_label_is_true[0].item() > 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def build_trec_eval(references, predictions, eval_field, filename, index, question):\n",
        "\n",
        "    def is_float(v):\n",
        "        \n",
        "        is_float = True\n",
        "        try:\n",
        "            f = float(v)\n",
        "        except Exception as notFloat:\n",
        "            is_float = False\n",
        "        return is_float\n",
        "\n",
        "    try:\n",
        "        entailment_score = 0\n",
        "        bm25_score = 0\n",
        "        semantic_score = 0\n",
        "        references[\"query\"] = [index]\n",
        "        references[\"q0\"] = [\"q0\"]\n",
        "        references[\"docid\"] = [filename]\n",
        "        references[\"rel\"] = [100]\n",
        "        eval_field = str(eval_field).replace(\"=\", \":\")\n",
        "        sind = eval_field.find(\"{\")\n",
        "        ind = eval_field.find(\"}\")\n",
        "        eval_field = eval_field[sind:ind + 1]\n",
        "        eval_field = eval_field.strip('\\n')\n",
        "        eval_field = eval_field.replace('\\n', '')\n",
        "        eval_field = eval_field.replace('\"\"', '\"')\n",
        "\n",
        "        if '\"' not in eval_field:\n",
        "            eval_field = eval_field.replace(':[', '\":[')\n",
        "            eval_field = eval_field.replace('{', '{\"')\n",
        "            eval_field = eval_field.replace('], ', '], \"')\n",
        "\n",
        "        eval_field = json.loads(eval_field, strict=False)\n",
        "\n",
        "    except Exception as nan:\n",
        "        print(f\"Exception {nan} {eval_field} {filename} {index}\" )\n",
        "        predictions[\"query\"] = [index]\n",
        "        predictions[\"q0\"] = [\"q0\"]\n",
        "        predictions[\"docid\"] = [\"None\"]\n",
        "        predictions[\"rank\"] = [0]\n",
        "        predictions[\"score\"] = [0]\n",
        "        predictions[\"system\"] = [question]\n",
        "\n",
        "        return references, predictions, entailment_score, bm25_score, semantic_score\n",
        "\n",
        "    lst_query = []\n",
        "    lst_q0 = []\n",
        "    lst_docid = []\n",
        "    lst_rank = []\n",
        "    lst_score = []\n",
        "    lst_system = []\n",
        "\n",
        "    for key in eval_field:\n",
        "        lst_query.append(index)\n",
        "        lst_q0.append(\"q0\")\n",
        "        lst_docid.append(str(key))\n",
        "        lst_rank.append(eval_field[key][3])\n",
        "\n",
        "        # Check if a semantic score\n",
        "        if is_float(eval_field[key][2]) == True:\n",
        "            lst_score.append(float(eval_field[key][2]))\n",
        "            semantic_score = float(eval_field[key][2])\n",
        "            bm25_score = float(eval_field[key][1])\n",
        "        else:\n",
        "            lst_score.append(float(eval_field[key][1]))\n",
        "            bm25_score = float(eval_field[key][1])\n",
        "        lst_system.append(question)\n",
        "\n",
        "        if len(eval_field[key]) > 4:\n",
        "            hyp = eval_field[key][4].strip()\n",
        "\n",
        "            # roBERTA Entailment\n",
        "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "            # run through model pre-trained on MNLI\n",
        "            x = nli_tokenizer.encode(question, hyp, return_tensors=\"pt\",\n",
        "                                        truncation=True)\n",
        "            logits = nli_model(x.to(device))[0]\n",
        "\n",
        "            # we throw away \"neutral\" (dim 1) and take the probability of\n",
        "            # \"entailment\" (2) as the probability of the label being true\n",
        "            entail_contradiction_logits = logits[:, [0, 2]]\n",
        "            probs = entail_contradiction_logits.softmax(dim=1)\n",
        "            prob_label_is_true = probs[:, 1]\n",
        "            entailment_score = prob_label_is_true[0].item()\n",
        "\n",
        "    predictions[\"query\"] = lst_query\n",
        "    predictions[\"q0\"] = lst_q0\n",
        "    predictions[\"docid\"] = lst_docid\n",
        "    predictions[\"rank\"] = lst_rank\n",
        "    predictions[\"score\"] = lst_score\n",
        "    predictions[\"system\"] = lst_system\n",
        "\n",
        "    return references, predictions, entailment_score, bm25_score, semantic_score\n",
        "\n",
        "def calculate_confidence_score(bm25_scores, semantic_scores):\n",
        "\n",
        "    try:\n",
        "\n",
        "        bm25_weight = 0.6\n",
        "        semantic_weight = 0.4\n",
        "\n",
        "        # Normalize BM25 scores using min-max normalization\n",
        "        min_bm25 = float(min(bm25_scores))\n",
        "        max_bm25 = float(max(bm25_scores))\n",
        "\n",
        "        if min_bm25 == 0 and max_bm25 == 0:\n",
        "            return [0]\n",
        "\n",
        "\n",
        "        normalized_bm25_scores = [(float(score) - min_bm25) / (max_bm25 - min_bm25) for score in bm25_scores]\n",
        "\n",
        "        # Normalize semantic ranking scores using min-max normalization\n",
        "        min_semantic = float(min(semantic_scores))\n",
        "        max_semantic = float(max(semantic_scores))\n",
        "        normalized_semantic_scores = [(float(score) - min_semantic) / (max_semantic - min_semantic) for score in\n",
        "                                        semantic_scores]\n",
        "\n",
        "        # Combine the normalized scores using weights\n",
        "        confidence_scores = [(bm25 * bm25_weight) + (semantic * semantic_weight)\n",
        "                                for bm25, semantic in zip(normalized_bm25_scores, normalized_semantic_scores)]\n",
        "\n",
        "    except Exception as ConfidenceError:\n",
        "        print(f\" Confidence Error {ConfidenceError}\")\n",
        "        return [0]\n",
        "\n",
        "    return confidence_scores\n",
        "\n",
        "semantic_search_references = {}\n",
        "semantic_search_predictions = {}\n",
        "semantic_search_map = []\n",
        "semantic_search_geo_map = []\n",
        "semantic_search_geo_rprec = []\n",
        "semantic_search_recip_rank = []\n",
        "semantic_search_entailment_score = []\n",
        "semantic_search_bm25_score = []\n",
        "semantic_search_semantic_score = []\n",
        "\n",
        "simple_search_references = {}\n",
        "simple_search_predictions = {}\n",
        "simple_search_map = []\n",
        "simple_search_geo_map = []\n",
        "simple_search_geo_rprec = []\n",
        "simple_search_recip_rank = []\n",
        "simple_search_entailment_score = []\n",
        "\n",
        "simple_search_rag_references = {}\n",
        "simple_search_rag_predictions = {}\n",
        "simple_search_rag_map = []\n",
        "simple_search_rag_geo_map = []\n",
        "simple_search_rag_geo_rprec = []\n",
        "simple_search_rag_recip_rank = []\n",
        "simple_search_rag_entailment_score = []\n",
        "\n",
        "semantic_search_rag_references = {}\n",
        "semantic_search_rag_predictions = {}\n",
        "semantic_search_rag_map = []\n",
        "semantic_search_rag_geo_map = []\n",
        "semantic_search_rag_geo_rprec = []\n",
        "semantic_search_rag_recip_rank = []\n",
        "semantic_search_rag_entailment_score = []\n",
        "semantic_search_rag_bm25_score = []\n",
        "semantic_search_rag_semantic_score = []\n",
        "\n",
        "simple_search_hybrid_references = {}\n",
        "simple_search_hybrid_predictions = {}\n",
        "simple_search_hybrid_map = []\n",
        "simple_search_hybrid_geo_map = []\n",
        "simple_search_hybrid_geo_rprec = []\n",
        "simple_search_hybrid_recip_rank = []\n",
        "simple_search_hybrid_entailment_score = []\n",
        "\n",
        "semantic_search_hybrid_references = {}\n",
        "semantic_search_hybrid_predictions = {}\n",
        "semantic_search_hybrid_map = []\n",
        "semantic_search_hybrid_geo_map = []\n",
        "semantic_search_hybrid_geo_rprec = []\n",
        "semantic_search_hybrid_recip_rank = []\n",
        "semantic_search_hybrid_entailment_score = []\n",
        "semantic_search_hybrid_bm25_score = []\n",
        "semantic_search_hybrid_semantic_score = []\n",
        "\n",
        "simple_search_hybrid_rag_references = {}\n",
        "simple_search_hybrid_rag_predictions = {}\n",
        "simple_search_hybrid_rag_map = []\n",
        "simple_search_hybrid_rag_geo_map = []\n",
        "simple_search_hybrid_rag_geo_rprec = []\n",
        "simple_search_hybrid_rag_recip_rank = []\n",
        "simple_search_hybrid_rag_entailment_score = []\n",
        "\n",
        "semantic_search_hybrid_rag_references = {}\n",
        "semantic_search_hybrid_rag_predictions = {}\n",
        "semantic_search_hybrid_rag_map = []\n",
        "semantic_search_hybrid_rag_geo_map = []\n",
        "semantic_search_hybrid_rag_geo_rprec = []\n",
        "semantic_search_hybrid_rag_recip_rank = []\n",
        "semantic_search_hybrid_rag_entailment_score = []\n",
        "semantic_search_hybrid_rag_bm25_score = []\n",
        "semantic_search_hybrid_rag_semantic_score = []\n",
        "\n",
        "for index, row in enumerate(dfq.itertuples()):\n",
        "    print(f\"Processing {index} of {len(dfq)}\")\n",
        "\n",
        "    # Semantic Search Step 1\n",
        "    semantic_search_references, semantic_search_predictions, entailment_score, bm25_score, semantic_score = build_trec_eval(\n",
        "        semantic_search_references, semantic_search_predictions, row.SemanticSearch, row.filename, index,\n",
        "        row.question)\n",
        "    results = trec_eval.compute(predictions=[semantic_search_predictions], references=[semantic_search_references])\n",
        "    # confidence score\n",
        "    semantic_search_bm25_score.append(bm25_score)\n",
        "    semantic_search_semantic_score.append(semantic_score)\n",
        "\n",
        "    semantic_search_map.append(float(results['map']))\n",
        "    semantic_search_geo_map.append(float(results['gm_map']))\n",
        "    semantic_search_geo_rprec.append(float(results['bpref']))\n",
        "    semantic_search_recip_rank.append(float(results['recip_rank']))\n",
        "    semantic_search_entailment_score.append(float(entailment_score))\n",
        "    # confidence score\n",
        "\n",
        "    # Simple Search Step 2\n",
        "    simple_search_references, simple_search_predictions, entailment_score, _, _ = build_trec_eval(\n",
        "        simple_search_references, simple_search_predictions, row.SimpleSearch, row.filename, index, row.question)\n",
        "    results = trec_eval.compute(predictions=[simple_search_predictions], references=[simple_search_references])\n",
        "\n",
        "    simple_search_map.append(float(results['map']))\n",
        "    simple_search_geo_map.append(float(results['gm_map']))\n",
        "    simple_search_geo_rprec.append(float(results['bpref']))\n",
        "    simple_search_recip_rank.append(float(results['recip_rank']))\n",
        "    simple_search_entailment_score.append(float(entailment_score))\n",
        "\n",
        "    # Simple Search RAG Step 3\n",
        "    simple_search_rag_references, simple_search_rag_predictions, entailment_score, _, _ = build_trec_eval(\n",
        "        simple_search_rag_references, simple_search_rag_predictions, row.SimpleSearchRAG, row.filename, index,\n",
        "        row.question)\n",
        "    results = trec_eval.compute(predictions=[simple_search_rag_predictions],\n",
        "                                references=[simple_search_rag_references])\n",
        "\n",
        "    simple_search_rag_map.append(float(results['map']))\n",
        "    simple_search_rag_geo_map.append(float(results['gm_map']))\n",
        "    simple_search_rag_geo_rprec.append(float(results['bpref']))\n",
        "    simple_search_rag_recip_rank.append(float(results['recip_rank']))\n",
        "    simple_search_rag_entailment_score.append(float(entailment_score))\n",
        "\n",
        "    # Semantic Search RAG Step 4\n",
        "    semantic_search_rag_references, semantic_search_rag_predictions, entailment_score, bm25_score, semantic_score = build_trec_eval(\n",
        "        semantic_search_rag_references, semantic_search_rag_predictions, row.SemanticSearchRAG, row.filename, index,\n",
        "        row.question)\n",
        "    results = trec_eval.compute(predictions=[simple_search_rag_predictions],\n",
        "                                references=[semantic_search_rag_references])\n",
        "    # #confidence score\n",
        "    semantic_search_rag_bm25_score.append(bm25_score)\n",
        "    semantic_search_rag_semantic_score.append(semantic_score)\n",
        "\n",
        "    semantic_search_rag_map.append(float(results['map']))\n",
        "    semantic_search_rag_geo_map.append(float(results['gm_map']))\n",
        "    semantic_search_rag_geo_rprec.append(float(results['bpref']))\n",
        "    semantic_search_rag_recip_rank.append(float(results['recip_rank']))\n",
        "    semantic_search_rag_entailment_score.append(float(entailment_score))\n",
        "\n",
        "    # Simple Hybrid Search Step 5\n",
        "    simple_search_hybrid_references, simple_search_hybrid_predictions, entailment_score, _, _ = build_trec_eval(\n",
        "        simple_search_hybrid_references, simple_search_hybrid_predictions, row.SimpleHybridSearch, row.filename,\n",
        "        index, row.question)\n",
        "    results = trec_eval.compute(predictions=[simple_search_hybrid_predictions],\n",
        "                                references=[simple_search_hybrid_references])\n",
        "\n",
        "    simple_search_hybrid_map.append(float(results['map']))\n",
        "    simple_search_hybrid_geo_map.append(float(results['gm_map']))\n",
        "    simple_search_hybrid_geo_rprec.append(float(results['bpref']))\n",
        "    simple_search_hybrid_recip_rank.append(float(results['recip_rank']))\n",
        "    simple_search_hybrid_entailment_score.append(float(entailment_score))\n",
        "\n",
        "    # Semantic Hybrid Search Step 6\n",
        "    semantic_search_hybrid_references, semantic_search_hybrid_predictions, entailment_score, bm25_score, semantic_score = build_trec_eval(\n",
        "        semantic_search_hybrid_references, semantic_search_hybrid_predictions, row.SemanticHybridSearch,\n",
        "        row.filename, index, row.question)\n",
        "    results = trec_eval.compute(predictions=[semantic_search_hybrid_predictions],\n",
        "                                references=[semantic_search_hybrid_references])\n",
        "    # confidence score\n",
        "    semantic_search_hybrid_bm25_score.append(bm25_score)\n",
        "    semantic_search_hybrid_semantic_score.append(semantic_score)\n",
        "\n",
        "    semantic_search_hybrid_map.append(float(results['map']))\n",
        "    semantic_search_hybrid_geo_map.append(float(results['gm_map']))\n",
        "    semantic_search_hybrid_geo_rprec.append(float(results['bpref']))\n",
        "    semantic_search_hybrid_recip_rank.append(float(results['recip_rank']))\n",
        "    semantic_search_hybrid_entailment_score.append(float(entailment_score))\n",
        "\n",
        "    # Simple Hybrid Search RAG Step 7\n",
        "    simple_search_hybrid_rag_references, simple_search_hybrid_rag_predictions, entailment_score, _, _ = build_trec_eval(\n",
        "        simple_search_hybrid_rag_references, simple_search_hybrid_rag_predictions, row.SimpleHybridSearchRAG,\n",
        "        row.filename, index, row.question)\n",
        "    results = trec_eval.compute(predictions=[simple_search_hybrid_rag_predictions],\n",
        "                                references=[simple_search_hybrid_rag_references])\n",
        "\n",
        "    simple_search_hybrid_rag_map.append(float(results['map']))\n",
        "    simple_search_hybrid_rag_geo_map.append(float(results['gm_map']))\n",
        "    simple_search_hybrid_rag_geo_rprec.append(float(results['bpref']))\n",
        "    simple_search_hybrid_rag_recip_rank.append(float(results['recip_rank']))\n",
        "    simple_search_hybrid_rag_entailment_score.append(float(entailment_score))\n",
        "\n",
        "    # Semantic Hybrid Search RAG Step 8\n",
        "    semantic_search_hybrid_rag_references, semantic_search_hybrid_rag_predictions, entailment_score, bm25_score, semantic_score = build_trec_eval(\n",
        "        semantic_search_hybrid_rag_references, semantic_search_hybrid_rag_predictions, row.SemanticHybridSearchRAG,\n",
        "        row.filename, index, row.question)\n",
        "    results = trec_eval.compute(predictions=[semantic_search_hybrid_rag_predictions],\n",
        "                                references=[semantic_search_hybrid_rag_references])\n",
        "    # confidence score\n",
        "    semantic_search_hybrid_rag_bm25_score.append(bm25_score)\n",
        "    semantic_search_hybrid_rag_semantic_score.append(semantic_score)\n",
        "\n",
        "    semantic_search_hybrid_rag_map.append(float(results['map']))\n",
        "    semantic_search_hybrid_rag_geo_map.append(float(results['gm_map']))\n",
        "    semantic_search_hybrid_rag_geo_rprec.append(float(results['bpref']))\n",
        "    semantic_search_hybrid_rag_recip_rank.append(float(results['recip_rank']))\n",
        "    semantic_search_hybrid_rag_entailment_score.append(float(entailment_score))\n",
        "\n",
        "print(f\"Semantic Search Results\")\n",
        "print(f\"Mean average precision: {np.mean(semantic_search_map)}\")\n",
        "print(f\"Geometric mean average precision: {np.mean(semantic_search_geo_map)}\")\n",
        "print(f\"Binary preference score: {np.mean(semantic_search_geo_rprec)}\")\n",
        "print(f\"Reciprocal rank: {np.mean(semantic_search_recip_rank)}\")\n",
        "semantic_search_confidence_score = 0\n",
        "semantic_search_confidence_score = calculate_confidence_score(semantic_search_bm25_score,\n",
        "                                                                semantic_search_semantic_score)\n",
        "print(f\"Confidence score: {np.mean(semantic_search_confidence_score)}\")\n",
        "\n",
        "print(f\"-----------------------------------------\")\n",
        "\n",
        "print(f\"Simple Search Results\")\n",
        "print(f\"Mean average precision: {np.mean(simple_search_map)}\")\n",
        "print(f\"Geometric mean average precision: {np.mean(simple_search_geo_map)}\")\n",
        "print(f\"Binary preference score: {np.mean(simple_search_geo_rprec)}\")\n",
        "print(f\"Reciprocal rank: {np.mean(simple_search_recip_rank)}\")\n",
        "print(f\"-----------------------------------------\")\n",
        "\n",
        "print(f\"Simple Search RAG Results\")\n",
        "print(f\"Mean average precision: {np.mean(simple_search_rag_map)}\")\n",
        "print(f\"Geometric mean average precision: {np.mean(simple_search_rag_geo_map)}\")\n",
        "print(f\"Binary preference score: {np.mean(simple_search_rag_geo_rprec)}\")\n",
        "print(f\"Reciprocal rank: {np.mean(simple_search_rag_recip_rank)}\")\n",
        "print(f\"Entailment score: {np.mean(simple_search_rag_entailment_score)}\")\n",
        "print(f\"-----------------------------------------\")\n",
        "\n",
        "print(f\"Semantic Search RAG Results\")\n",
        "print(f\"Mean average precision: {np.mean(semantic_search_rag_map)}\")\n",
        "print(f\"Geometric mean average precision: {np.mean(semantic_search_rag_geo_map)}\")\n",
        "print(f\"Binary preference score: {np.mean(semantic_search_rag_geo_rprec)}\")\n",
        "print(f\"Reciprocal rank: {np.mean(semantic_search_rag_recip_rank)}\")\n",
        "print(f\"Entailment score: {np.mean(semantic_search_rag_entailment_score)}\")\n",
        "semantic_search_rag_confidence_score = 0\n",
        "semantic_search_rag_confidence_score = calculate_confidence_score(semantic_search_rag_bm25_score,\n",
        "                                                                    semantic_search_rag_semantic_score)\n",
        "print(f\"Confidence score: {np.mean(semantic_search_rag_confidence_score)}\")\n",
        "print(f\"-----------------------------------------\")\n",
        "\n",
        "print(f\"Simple Hybrid Search Results\")\n",
        "print(f\"Mean average precision: {np.mean(simple_search_hybrid_map)}\")\n",
        "print(f\"Geometric mean average precision: {np.mean(simple_search_hybrid_geo_map)}\")\n",
        "print(f\"Binary preference score: {np.mean(simple_search_hybrid_geo_rprec)}\")\n",
        "print(f\"Reciprocal rank: {np.mean(simple_search_hybrid_recip_rank)}\")\n",
        "print(f\"-----------------------------------------\")\n",
        "\n",
        "print(f\"Semantic Hybrid Search Results\")\n",
        "print(f\"Mean average precision: {np.mean(semantic_search_hybrid_map)}\")\n",
        "print(f\"Geometric mean average precision: {np.mean(semantic_search_hybrid_geo_map)}\")\n",
        "print(f\"Binary preference score: {np.mean(semantic_search_hybrid_geo_rprec)}\")\n",
        "print(f\"Reciprocal rank: {np.mean(semantic_search_hybrid_recip_rank)}\")\n",
        "semantic_search_hybrid_confidence_score = 0\n",
        "semantic_search_hybrid_confidence_score = calculate_confidence_score(semantic_search_hybrid_bm25_score,\n",
        "                                                                        semantic_search_hybrid_semantic_score)\n",
        "print(f\"Confidence score: {np.mean(semantic_search_hybrid_confidence_score)}\")\n",
        "print(f\"-----------------------------------------\")\n",
        "\n",
        "print(f\"Simple Hybrid Search RAG Results\")\n",
        "print(f\"Mean average precision: {np.mean(simple_search_hybrid_rag_map)}\")\n",
        "print(f\"Geometric mean average precision: {np.mean(simple_search_hybrid_rag_geo_map)}\")\n",
        "print(f\"Binary preference score: {np.mean(simple_search_hybrid_rag_geo_rprec)}\")\n",
        "print(f\"Reciprocal rank: {np.mean(simple_search_hybrid_rag_recip_rank)}\")\n",
        "print(f\"Entailment score: {np.mean(simple_search_hybrid_rag_entailment_score)}\")\n",
        "print(f\"-----------------------------------------\")\n",
        "\n",
        "print(f\"Semantic Hybrid Search RAG Results\")\n",
        "print(f\"Mean average precision: {np.mean(semantic_search_hybrid_rag_map)}\")\n",
        "print(f\"Geometric mean average precision: {np.mean(semantic_search_hybrid_rag_geo_map)}\")\n",
        "print(f\"Binary preference score: {np.mean(semantic_search_hybrid_rag_geo_rprec)}\")\n",
        "print(f\"Reciprocal rank: {np.mean(semantic_search_hybrid_rag_recip_rank)}\")\n",
        "print(f\"Entailment score: {np.mean(semantic_search_hybrid_rag_entailment_score)}\")\n",
        "semantic_search_hybrid_rag_confidence_score = 0\n",
        "semantic_search_hybrid_rag_confidence_score = calculate_confidence_score(semantic_search_hybrid_rag_bm25_score,\n",
        "                                                                            semantic_search_hybrid_rag_semantic_score)\n",
        "print(f\"Confidence score: {np.mean(semantic_search_hybrid_rag_confidence_score)}\")\n",
        "print(f\"-----------------------------------------\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Semantic Search Results\n",
        "Mean average precision: 0.6289009497964722\n",
        "Geometric mean average precision: 0.6289036635006784\n",
        "Binary preference score: 0.0\n",
        "Reciprocal rank: 0.6289009497964722\n",
        "Confidence score: 0.32117506105155313\n",
        "-----------------------------------------\n",
        "Simple Search Results\n",
        "Mean average precision: 0.5637720488466758\n",
        "Geometric mean average precision: 0.5637752374491181\n",
        "Binary preference score: 0.0\n",
        "Reciprocal rank: 0.5637720488466758\n",
        "-----------------------------------------\n",
        "Simple Search RAG Results\n",
        "Mean average precision: 0.5569877883310719\n",
        "Geometric mean average precision: 0.5569910583446404\n",
        "Binary preference score: 0.0\n",
        "Reciprocal rank: 0.5569877883310719\n",
        "Entailment score: 0.15832776706789012\n",
        "-----------------------------------------\n",
        "Semantic Search RAG Results\n",
        "Mean average precision: 0.5569877883310719\n",
        "Geometric mean average precision: 0.5569910583446404\n",
        "Binary preference score: 0.0\n",
        "Reciprocal rank: 0.5569877883310719\n",
        "Entailment score: 0.1549185791918322\n",
        "Confidence score: 0.34570918379938587\n",
        "-----------------------------------------\n",
        "Simple Hybrid Search Results\n",
        "Mean average precision: 0.5936227951153324\n",
        "Geometric mean average precision: 0.5936259701492538\n",
        "Binary preference score: 0.0\n",
        "Reciprocal rank: 0.5936227951153324\n",
        "-----------------------------------------\n",
        "Semantic Hybrid Search Results\n",
        "Mean average precision: 0.6264133876074174\n",
        "Geometric mean average precision: 0.6264161148801447\n",
        "Binary preference score: 0.0\n",
        "Reciprocal rank: 0.6264133876074174\n",
        "Confidence score: 0.49504462748491823\n",
        "-----------------------------------------\n",
        "Simple Hybrid Search RAG Results\n",
        "Mean average precision: 0.586386250565355\n",
        "Geometric mean average precision: 0.5863895070104025\n",
        "Binary preference score: 0.0\n",
        "Reciprocal rank: 0.586386250565355\n",
        "Entailment score: 0.1562549940426834\n",
        "-----------------------------------------\n",
        "Semantic Hybrid Search RAG Results\n",
        "Mean average precision: 0.6169154228855721\n",
        "Geometric mean average precision: 0.6169182451379466\n",
        "Binary preference score: 0.0\n",
        "Reciprocal rank: 0.6169154228855721\n",
        "Entailment score: 0.15223597163569244\n",
        "Confidence score: 0.5658654374165991\n",
        "-----------------------------------------"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
